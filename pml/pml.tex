\documentclass{myclass}
\usepackage[polish]{babel}

% My notes on ML and related concepts
\author{Bartosz Hanc}

\tikzset{%
  every neuron/.style={ circle, draw, minimum size=1cm }, neuron missing/.style={ draw=none,
    scale=4, text height=0.333cm, execute at begin node=\color{black}$\vdots$ }, }

\begin{document}
\tableofcontents
\newpage

\section{Rachunek prawdopodobieństwa}

\subsection{Definicja przestrzeni probabilistycznej}

Rozkładem prawdopodobieństwa \(P\) w pewnym zbiorze zdarzeń elementarnych \(\Omega \neq \emptyset\)
nazywamy odwzorowanie
\begin{equation*}
    P: \Sigma \mapsto [0;1]\,,
\end{equation*}
gdzie \(\Sigma\) jest rodziną podzbiorów \(\Omega\) (inaczej rodziną zdarzeń) taką, że
\begin{equation*}
    \Omega \in \Sigma\,,\quad A \in \Sigma \implies A' \in \Sigma\,,\quad \forall A_1, A_2, \ldots \in \Sigma : \bigcup_{i}
    A_i \in \Sigma\,,
\end{equation*}
które spełnia: \(P(\Omega) = 1\) oraz dla dowolnych parami rozłącznych zdarzeń \(A_1, A_2, \ldots\)
należących do \(\Sigma\) zachodzi
\begin{equation*}
    P\left(\bigcup_i A_i\right) = \sum_i P(A_i)\,.
\end{equation*}
Trójkę \((\Omega, \Sigma, P)\) nazywamy przestrzenią probabilistyczną. Z powyższej definicji
wynikają znane własności prawdopodobieństwa tj. \(P(A') = 1 - P(A)\) oraz \(P(A \cup B) = P(A) +
P(B) - P(A , B)\). 

\subsection{Prawdopodobieństwo warunkowe}

Definiujemy również prawdopodobieństwo warunkowe zdarzenia \(A\) pod warunkiem zdarzenia \(B\) o
dodatnim prawdopodobieństwie 
\begin{equation*}
    P(A \mid B) := \frac{P(A , B)}{P(B)}\,.
\end{equation*}
Na podstawie powyższej definicji definiujemy niezależność zdarzeń \(A\), \(B\) jako własność
\(P(A,B) = P(A)P(B)\), co dla zdarzenia \(B\) o dodatnim prawdopodobieństwie jest równoważne z \(P(A
\mid B) = P(A)\). Ponadto jeśli zdarzenia \(A_1, A_2, \ldots \in \Sigma\) są parami rozłączne i
zachodzi \(\bigcup_i A_i = \Omega\) to dla dowolnego zdarzenia \(B \in \Sigma\) możemy zapisać
\begin{equation*}
    P(B) = \sum_i P(B \mid A_i) P(A_i)\,.
\end{equation*}
Z definicji prawdopodobieństwa warunkowego trywialnie udowodnić twierdzenie Bayesa
\begin{equation*}
    P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}\,.
\end{equation*}

\subsection{Zmienne losowe}

W uczeniu maszynowym będą interesować nas zmienne o wartościach w \(\mathbb{R}^n\). Zmienne takie
nazywamy zmiennymi losowymi wielowymiarowymi i definiujemy jako odwzorowania
\begin{equation*}
    X: \Omega \mapsto \mathbb{R}^n
\end{equation*}
takie, że dla każdego \(A \subseteq \mathbb{R}^n\) zbiór \(\{\omega \in \Omega \mid X(\omega) \in
A\}\) należy do rodziny zdarzeń \(\Sigma\). Przy takiej definicji prawdopodobieństwo, iż zmienna
\(X\) ma wartość należącą do pewnego przedziału \(A\) wynosi
\begin{equation*}
    P(X \in A) = P(\{\omega \in \Omega \mid X(\omega) \in A\})\,.
\end{equation*}

Dowolny rozkład prawdopodobieństwa zmiennej losowej \(n\)--wymiarowej \(X = (X_1, X_2, \ldots,
X_n)\) jest wyznaczony jednoznacznie przez zadanie funkcji \(F(\mathbf{x}): \mathbb{R}^n \mapsto
[0;1]\) zwanej dystrybuantą zdefiniowanej jako
\begin{equation*}
    F(\mathbf{x}) = F(x_1, \ldots, x_n) := P(X_1 \leq x_1, \ldots, X_n \leq x_n)\,.
\end{equation*}
Zasadniczo będą nas interesować jednak dwa przypadki rozkładów prawdopodobieństwa zmiennych
losowych: rozkłady dyskretne i rozkłady ciągłe. W przypadku rozkładu dyskretnego istnieje pewien
przeliczalny zbiór \(S \subset \mathbb{R}^n\) taki, że \(P(X \in S) = 1\). Rozkład ten jest zadany
jednoznacznie przez podanie \(|S|\) liczb \(p_i > 0\) określających prawdopodobieństwa \(p_i = P(X =
\mathbf{x}_i)\) dla wszystkich \(\mathbf{x}_i \in S\). W przypadku rozkładu ciągłego istnieje z
kolei funkcja \(p(\mathbf{x}): \mathbb{R}^n \mapsto [0; \infty)\) taka, że
\begin{equation*}
    P(X_1 \in [a_1; b_1], \ldots, X_n \in [a_n; b_n]) = \int\limits_{a_1}^{b_1}\cdots\int\limits_{a_n}^{b_n}p(\mathbf{x})\dd[n]{\mathbf{x}}\,.
\end{equation*}
Funkcje \(p(\mathbf{x})\) nazywamy gęstością prawdopodobieństwa. W obu przypadkach musi być
spełniony warunek unormowania postaci odpowiednio
\begin{equation*}
    \sum_i p_i = 1\,,\quad \int\limits_{\mathbb{R}^n}p(\mathbf{x})\dd[n]{\mathbf{x}} = 1\,.
\end{equation*}

Będziemy często wykorzystywać wartość oczekiwaną pewnej funkcji \(f(\mathbf{x})\) zmiennej losowej
\(X\) zdefiniowaną odpowiednio dla rozkładu \(p\) -- dyskretnego lub ciągłego jako
\begin{equation*}
    \mathbb{E}[f(\mathbf{x})] := \sum_{\mathbf{x}_i \in S} f(\mathbf{x}_i)p_i \cong \int\limits_{\mathbb{R}^n}f(\mathbf{x})p(\mathbf{x})\dd[n]{\mathbf{x}}\,.
\end{equation*}
Zauważmy przy tym, iż funkcja \(f(\mathbf{x})\) może być zupełnie dowolna, np. dla funkcji
charakterystycznej (indykatorowej) zbioru \(A \subset \mathbb{R}^n\) \(f(\mathbf{x}) =
\mathcal{I}_A\) mamy \(\mathbb{E}[\mathcal{I}_A(\mathbf{x})] = P(X \in A)\) lub dla iloczynu funkcji
Heaviside'a \(f(\mathbf{x}) = \theta(t_1 - x_1)\cdots\theta(t_n - x_n)\) mamy
\(\mathbb{E}[f(\mathbf{x})] = F(t_1,\ldots, t_n)\).

\subsection{Rozkłady brzegowe}

Niech \(X = (X_1, \ldots, X_n)\) będzie \(n\)--wymiarową zmienną losową o dystrybuancie
\(F(\mathbf{x})\). Rozkład brzegowy względem \(k\) zmiennych \(X_{\sigma(1)},\ldots,X_{\sigma(k)}\)
definiujemy jako rozkład wyznaczony przez dystrybuantę
\begin{equation*}
    F_{X_{\sigma(1)},\ldots,X_{\sigma(k)}} (x_{\sigma(1)},\ldots,x_{\sigma(k)}) := \lim_{x_{\sigma(k+1)}\to\infty,\ldots,x_{\sigma(n)}\to\infty} F(x_1,\ldots,x_n)\,.
\end{equation*}

\subsection{Zmienne losowe niezależne}
Niech \(X = (X_1, \ldots, X_k)\) będzie \(n\)--wymiarową zmienną losową o rozkładzie wyznaczonym
przez dystrybuantę \(F(\mathbf{x})\). Powiemy, iż zmienne losowe \(n_1,\ldots,n_k\) -- wymiarowych
(\(n_1 + \ldots + n_k = n\)) \(X_1, \ldots, X_k\) są niezależne iff dla dowolnych
\(\mathbf{x}_1\in\mathbb{R}^{n_1},\ldots,\mathbf{x}_k\in\mathbb{R}^{n_k}\) zachodzi
\begin{equation*}
    F(\mathbf{x}_1,\ldots,\mathbf{x}_k) = F_{X_1}(\mathbf{x}_1)\cdot\ldots\cdot F_{X_k}(\mathbf{x}_k)\,.
\end{equation*}

\subsection{Funkcja tworząca momenty}

Funkcją tworzącą momenty (z ang. \textit{moment generating function}) nazywamy funkcję określoną
wzorem
\begin{equation*}
    M_X(t) := \mathbb{E}\left[\e^{t X}\right]
\end{equation*}
dla zmiennej losowej rzeczywistej \(X: \Sigma \mapsto \mathbb{R}\). Powyższa wielkość jest określona
zawsze tylko dla \(t=0\). Dla innych \(t\), \(M_X(t)\) może w ogólności nie istnieć. MGF używamy,
aby łatwiej obliczać momenty różnych rzędów. Istotnie jeśli \(M_X(t)\) istnieje w pewnym otoczeniu
\(t=0\) to
\begin{equation*}
    \mathbb{E}[X^k] = \dv[k]{M_X}{t}\Big|_{t=0}\,.
\end{equation*}
Jednocześnie łatwo pokazać, że zachodzi \(M_X(at) = M_{aX}(t)\) oraz \(M_{X+b}(t) = M_X(t)\e^{tb}\).

\subsection{Funkcja charakterystyczna}

Funkcję charakterystyczną rozkładu o gęstości \(p(x)\) definiujemy jako
\begin{equation*}
    \varphi_X(x) = \mathbb{E}\left[\e^{\im t x}\right] = \int\limits_{-\infty}^{+\infty} p(x)\e^{\im tx} \dd{x}\,.
\end{equation*}
Widzimy, iż jest to zatem transformata Fouriera funkcji gęstości. Funkcja charakterystyczna koduje
pełną informację o rozkładzie i możemy wyciągną z niej funkcję gęstości przez zastosowanie odwrotnej
transformacji Fouriera.

\subsection{Rozkłady warunkowe}

W ogólnym przypadku zmiennej losowej \(n\) -- wymiarowej \(Z = (Z_1, \ldots, Z_n)\) o ciągłym
rozkładzie \(p(\mathbf{z})\) jeśli wydzielimy zmienne \(k\) i \(n-k\) -- wymiarowe \(X =
(Z_{\sigma(1)}, \ldots, Z_{\sigma(k)})\), \(Y = (Z_{\sigma(k+1)}, \ldots, Z_{\sigma(n)})\) to
rozkład warunkowy zmiennej \(X \mid Y\) definiujemy jako rozkład zadany przez gęstość
prawdopodobieństwa
\begin{equation*}
    p(\mathbf{x} \mid \mathbf{y}) := \frac{p(\mathbf{z})}{p_Y(\mathbf{y})} = \frac{p(\mathbf{x},\mathbf{y})}{p_Y(\mathbf{y})}\,.
\end{equation*}

\subsection{Transformacja zmiennych wielowymiarowych}

Niech \(X = (X_1, \ldots, X_n)\) będzie zmienną losową wielowymiarową o rozkładzie ciągłym o
gęstości \(p_X(\mathbf{x})\). Rozważmy bijekcję \((X_1, \ldots, X_n) \mapsto (Y_1, \ldots, Y_n)\).
Chcemy znaleźć wyrażenie na gęstość \(p_Y(\mathbf{y})\) w nowych zmiennych. Ponieważ infinitezymalne
prawdopodobieństwo jest niezmiennicze względem zmiany współrzędnych więc zachodzi
\begin{equation*}
    p_X(x_1,\ldots, x_n)\dd{x_1}\ldots\dd{x_n} = p_Y(y_1,\ldots, y_n)\dd{y_1}\ldots\dd{y_n}\,,
\end{equation*}
skąd
\begin{equation*}
    p_Y(y_1,\ldots,y_n) = \left|\frac{\partial(x_1,\ldots,x_n)}{\partial(y_1,\ldots,y_n)}\right|p_X(x_1(\mathbf{y}),\ldots,x_n(\mathbf{y}))\,.
\end{equation*}

\subsection{Macierz kowariancji}

Macierz kowariancji funkcji \(f(\mathbf{x})\) zmiennej losowej \(X\) definiujemy jako
\begin{equation*}
    \oper{\Sigma}[f(\mathbf{x})] := \mathbb{E}\left[(f(\mathbf{x}) - \boldsymbol{\mu}_f)(f(\mathbf{x}) - \boldsymbol{\mu}_f)^\top\right]\,,
\end{equation*}
gdzie \(\boldsymbol{\mu}_f = \mathbb{E}[f(\mathbf{x})]\). Elementy diagonalne
\(\mathsf{\Sigma}_{ii}\) tej macierzy nazywamy wariancjami zmiennych \(X_i\), natomiast elementy
pozadiagonalne \(\mathsf{\Sigma}_{ij}\) nazywamy kowariancjami zmiennych \(X_i\) i \(X_j\).
Oczywiście \(\oper{\Sigma}\) jest macierzą symetryczną. Nadto jeśli \(f\) jest funkcją
identycznościową tj. \(f(\mathbf{x}) = \mathbf{x}\) to \(\oper{\Sigma}\) jest macierzą nieujemnie
określoną, gdyż dla dowolnego \(\mathbf{v} \in \mathbb{R}^n\) mamy
\begin{equation*}
    \mathbf{v}^\top \oper{\Sigma} \mathbf{v} = \mathbb{E}[\mathbf{v}^\top (\mathbf{x} - \boldsymbol{\mu})(\mathbf{x} - \boldsymbol{\mu})^\top \mathbf{v}] = \mathbb{E}[z^2] \geq 0\,,
\end{equation*}
gdzie \(z = \mathbf{v}^\top (\mathbf{x} - \boldsymbol{\mu}) \in \mathbb{R}\). Jeśli \(X_1, \ldots,
X_n\) są niezależne i \(f\) jest funkcją identycznościową to \(\oper{\Sigma}\) jest macierzą
diagonalną.

\subsection{Wielowymiarowy rozkład normalny}

Jeśli zmienna wielowymiarowa \(X = (X_1, \ldots, X_n)\) ma wielowymiarowy rozkład normalny (z ang.
\textit{Multivariate Normal distribution -- MVN}) z wartością oczekiwaną \(\boldsymbol{\mu}\) i
macierzą kowariancji \(\oper{\Sigma}\), co oznaczamy jako \(X \sim \mathcal{N}(\boldsymbol{\mu},
\oper{\Sigma})\), to gęstość prawdopodobieństwa jest dana
\begin{equation*}
    \phi(\mathbf{x}) = \frac{1}{\sqrt{(2\pi)^n\det\oper{\Sigma}}}\exp\left\{-\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu})^\top\oper{\Sigma}^{-1}(\mathbf{x} - \boldsymbol{\mu})\right\}
\end{equation*}
Macierz \(\oper{\Lambda} = \oper{\Sigma}^{-1}\) nazywamy macierzą precyzji. Jeśli \(\mathbf{v}_i\)
są unormowanymi wektorami własnymi macierzy \(\oper{\Sigma}\), a \(\lambda_i\) odpowiadającymi im
wartościami własnymi i zakładając, iż widmo \(\{\lambda_i\}\) jest niezdegenerowane mamy z
twierdzenia spektralnego
\begin{equation*}
    \oper{\Lambda} = \sum_{i=1}^n\frac{1}{\lambda_i}\mathbf{v}_i\mathbf{v}_i^\top
\end{equation*}
oraz wiemy, iż wektory \(\{\mathbf{v}_i\}\) tworzą bazę ortonormalną przestrzeni \(\mathbb{R}^n\). Z
powyższego możemy zatem wyrazić wektor \(\mathbf{x} - \boldsymbol{\mu}\) jako kombinację liniową
wektorów \(\{\mathbf{v}_i\}\) tj.
\begin{equation*}
    \mathbf{x} - \boldsymbol{\mu} = \sum_{i=1}^n t_i\mathbf{v}_i\,,
\end{equation*}
co pozwala zapisać gęstość prawdopodobieństwa jako
\begin{equation*}
    \phi(t_1,\ldots,t_2) \cong \exp\left\{-\frac{1}{2}\sum_{i=1}^n\frac{t_i^2}{\lambda_i}\right\}\,.
\end{equation*}
Z powyższego wzoru widać, iż poziomice gęstości są wielowymiarowymi elipsoidami, których półosie są
skierowane wzdłuż wektorów własnych \(\oper{\Sigma}\) i mają długości proporcjonalne do
\(\sqrt{\lambda_i}\).

Powiemy, iż wielowymiarowa zmienna losowa \(X \sim \mathcal{N}(\boldsymbol{\mu}, \oper{\Sigma})\) ma
standardowy wielowymiarowy rozkład normalny jeśli \(\boldsymbol{\mu} = \mathbf{0}\) i
\(\oper{\Sigma} = \oper{1}\). Wówczas
\begin{equation*}
    \phi(\mathbf{x}) = \frac{1}{\sqrt{(2\pi)^n}}\exp\left\{-\frac{1}{2}\mathbf{x}^\top\mathbf{x}\right\}\,.
\end{equation*}

Można wykazać, iż jeśli \(X \sim \mathcal{N}(\boldsymbol{\mu}, \oper{\Sigma})\) dla
\(\oper{\Sigma}\) o niezdegenerowanym widmie to wszystkie rozkłady brzegowe i warunkowe \(X\) są
rozkładami normalnymi.

\subsection{Zbieżność w rachunku prawdopodobieństwa}

W rachunku prawdopodobieństwa definiujemy trzy zasadnicze rodzaje zbieżności ciągu zmiennych
losowych \((X_n)\). 
\begin{itemize}
    \item Ciąg \((X_n)\) jest zbieżny do \(X\) stochastycznie iff
    \begin{equation*}
        \forall \epsilon > 0: \lim_{n\to\infty} P(|X_n - X| < \epsilon) = 1\,.
    \end{equation*}

    \item Ciąg \((X_n)\) jest zbieżny do \(X\) z prawdopodobieństwem 1 iff
    \begin{equation*}
        P\left(\lim_{n\to\infty} X_n = X\right) = 1\,.
    \end{equation*}

    \item Ciąg \((X_n)\) \(n\)--wymiarowych zmiennych losowych jest zbieżny do \(X\) według
    dystrybuant iff
    \begin{equation*}
        \forall \mathbf{x} \in \mathbb{R}^n, F_X(\mathbf{x}) \text{ -- ciągła w \(\mathbf{x}\)}: \lim_{n\to\infty} F_{X_n}(\mathbf{x}) = F_X(\mathbf{x})
    \end{equation*}

\end{itemize}

Pomiędzy tak zdefiniowanymi rodzajami zbieżności zachodzą następujące implikacje:
\begin{enumerate}
    \item \(X_n \to X\) z prawdopodobieństwem 1 \(\implies\) \(X_n \to X\) stochastycznie
    \item \(X_n \to X\) stochastycznie \(\implies\) \(X_n \to X\) według dystrybuant
    \item \(X_n \to X\) stochastycznie \(\implies\) istnieje podciąg \((X_{n_k})\) zbieżny do \(X\)
    z prawdopodobieństwem 1
\end{enumerate}

\subsection{Rozkłady prawdopodobieństwa}
\subsubsection{Rozkład Bernoulliego}
Jeśli zmienna losowa ma wartości w zbiorze \(\{x_1, x_2\}\) oraz \(P(X = x_1) = p\) i \(P(X = x_2) =
1 - p\) (dla \(0\leq p \leq 1\)) to mówimy, że \(X \sim \text{Ber}(p)\) (zmienna \(X\) ma rozkład
Bernoulliego z parametrem \(p\)).

\subsubsection{Rozkład dwumianowy}
Próbą Bernoulliego nazywamy doświadczenie losowe, którego wynik \(X \sim \text{Ber}(p)\). Schematem
dwumianowym nazywamy \(n\)--krotne powtórzenie próby Bernoulliego przy założeniu, iż poszczególne
próby są niezależne. Jeśli \(S\) będzie zmienną losową o wartościach w \(\mathbb{N} \cup \{0\}\),
która opisuje liczbę sukcesów w schemacie dwumianowym długości \(n\). Wówczas
\begin{equation*}
    P(S = k) = {n \choose k} p^k (1 - p)^{n-k}
\end{equation*}
i mówimy, że \(S \sim \text{Bin}(n, p)\) (zmienna \(S\) ma rozkład dwumianowy z parametrami \(n,
p\)).

\subsubsection{Rozkład geometryczny}
Rozważamy schemat Bernoulliego o nieskończonej długości. Jeśli \(T\) będzie zmienną losową o
wartościach w \(\mathbb{N} \cup \{0\}\), która opisuje liczbę prób Bernoulliego z parametrem \(p\)
do momentu uzyskania pierwszego sukcesu to
\begin{equation*}
    P(T=k) = (1-p)^{k-1}p
\end{equation*}
i mówimy, że \(T \sim \text{Geo}(p)\) (zmienna \(T\) ma rozkład geometryczny z parametrem \(p\)).

\subsubsection{Rozkład Poissona}
Załóżmy, iż mamy dany skończony przedział czasowy \([0; \tau]\) dla pewnego \(\tau\). Niech zmienna
losowa \(N\) o wartościach w \(\mathbb{N} \cup \{0\}\) opisuje liczbę wystąpień pewnego zdarzenia w
tym przedziale, przy czym
\begin{itemize}
    \item zdarzenia występują niezależnie od siebie
    \item intensywność wystąpień jest stała
\end{itemize}
to 
\begin{equation*}
    P(N = k) = \frac{\e^{-\lambda}\lambda^k}{k!}
\end{equation*}
i mówimy, że \(N \sim \text{Pois}(\lambda)\) (zmienna \(N\) ma rozkład Poissona z parametrem
\(\lambda\)). Zachodzi ponadto \textbf{twierdzenie Poissona}: niech \((S_n)\) będzie ciągiem takim,
że \(S_n \sim \text{Bin}(n, p_n)\), gdzie ciąg \((p_n)\) jest taki, iż \(\lim_{n \to \infty} np_n =
\lambda\), wówczas 
\begin{equation*}
\lim_{n\to\infty}P(S_n = k) =\frac{\e^\lambda\lambda^k}{k!}\,.
\end{equation*}

\subsubsection{Rozkład jednostajny}
Jeśli zmienna losowa \(X\) o wartościach rzeczywistych ma gęstość prawdopodobieństwa daną przez
\begin{equation*}
    p(x) = \begin{cases}
        \frac{1}{b - a}\,,\quad &x \in [a;b]\\
        0\,, &x \notin [a;b]
    \end{cases}
\end{equation*}
to mówimy \(X \sim \mathcal{U}(a,b)\) (zmienna \(X\) ma rozkład jednostajny na odcinku \([a;b]\)).

\subsubsection{Rozkład wykładniczy}
Jeśli zmienna losowa \(T\) o wartościach rzeczywistych opisuje prawdopodobieństwo uzyskania
pierwszego zdarzenia po czasie \(x\) modelowanego przez rozkład \(\text{Pois}(\lambda)\) to
\begin{equation*}
    p(x) = \begin{cases}
        \lambda \e^{-\lambda x}\,, &x \geq 0\\
        0\,, &x < 0 
    \end{cases}
\end{equation*}
i mówimy \(T \sim \text{Exp}(\lambda)\) (zmienna \(T\) ma rozkład wykładniczy z parametrem
\(\lambda\)).

\subsubsection{Rozkład gamma}
Mówimy, że zmienna \(X\) o wartościach rzeczywistych ma rozkład gamma z parametrami \(p, a > 0\) tj.
\(X \sim \Gamma(p, a)\) iff gęstość prawdopodobieństwa ma postać
\begin{equation*}
    p(x) = \begin{cases}
        \frac{a^p}{\Gamma(p)}x^{p-1}\e^{-ax}\,, &x > 0\\
        0\,, &x \leq 0
    \end{cases}\quad,
\end{equation*}
gdzie \(\Gamma\) to funkcja gamma Eulera. Parametr \(p\) nazywamy parametrem kształtu, a \(a\) --
parametrem intensywności. Szczególnym przypadkiem rozkładu gamma jest rozkład \(\chi^2\)
zdefiniowany jako rozkład \(\chi^2(n) := \Gamma(n/2, 1/2)\).

\subsection{Elementarz teorii informacji}

\subsubsection{Definicja i własności entropii}

Mając dany skończony zbiór zdarzeń elementarnych \(\{A_1,\ldots,A_n\}\) taki, że wynikiem
eksperymentu losowego może być dokładnie jedno z nich oraz prawdopodobieństwa \(p_1,\ldots,p_n\),
\(\sum_{i}p_i = 1\) każdego z nich powiemy, iż
\begin{equation*}
    A := \mqty(A_1 & A_2 & \cdots & A_n \\ p_1 & p_2 & \cdots & p_n)
\end{equation*} 
jest \textit{schematem skończonym} (z ang. \textit{finite scheme}). Przykładowo rzut sprawiedliwą,
sześcienną kostką do gry jest opisany przez schemat
\begin{equation*}
    \mqty(A_1 & A_2 & A_3 & A_4 & A_5 & A_6 \\ 1/6 & 1/6 & 1/6 & 1/6 & 1/6 & 1/6) \,.
\end{equation*} 
Zauważmy, że każdy schemat skończony opisuje pewną \textit{niepewność} dotyczącą doświadczenia
losowego. Przykładowo jest oczywiste, iż dla schematów
\begin{equation*}
    \mqty(A_1 & A_2 \\ 0.99 & 0.01)\,,\quad \mqty(A_1 & A_2 \\ 0.5 & 0.5)
\end{equation*}
pierwszy z nich opisuje znacznie mniejszą niepewność od drugiego, gdyż prawie z pewnością wynikiem
eksperymentu losowego będzie \(A_1\). Wprowadzimy teraz wielkość, która w sensowny sposób mierzy
ilość niepewności w danym schemacie skończonym. Wielkością taką jest \textit{entropia Shannona}
zdefiniowana dla schematu
\begin{equation*}
    A = \mqty(A_1 & A_2 & \cdots & A_n \\ p_1 & p_2 & \cdots & p_n)
\end{equation*}
jako
\begin{equation*}
    H(A) = H(p_1,p_2,\ldots,p_n) := -\sum_{i=1}^n p_i\lg p_i\,,
\end{equation*}
gdzie możemy wybrać dowolną ustaloną podstawę logarytmu oraz stwierdzamy, iż jeśli \(p_k = 0\) to
\(p_k \lg p_k = 0\). Jeśli jako podstawę wybierzemy liczbę 2 to entropię mierzymy w \textit{bitach}
tj. 1 bit jest to ilość niepewności zawarta w schemacie skończonym o  dwóch jednakowo
prawdopodobnych wynikach
\begin{equation*}
    H = -\log_2\frac{1}{2} = 1\,.
\end{equation*}
Przekonamy się teraz, iż tak zdefiniowana miara niepewności ma szereg własności, których
spodziewalibyśmy się dla sensownej miary niepewności. Zauważmy wpierw, iż \(H(p_1,\ldots,p_n) = 0\)
iff dokładnie jedno zdarzenie \(A_k \in A\) jest pewne, a pozostałe niemożliwe. Zauważmy dodatkowo,
iż z nierówności Jensena mamy dla funkcji wypukłej \(\phi(x) = x \lg x\)
\begin{equation*}
    \phi\left(\sum_{i=1}^n \lambda_i x_i\right) \leq \sum_{i=1}^n \lambda_i \phi(x_i)\,,
\end{equation*}
dla dowolnych \(x_1,\ldots,x_n \in \mathbb{R}\) i \(\lambda_1,\ldots,\lambda_n \in [0;1]\), \(\sum_i
\lambda_i = 1\), skąd
\begin{equation*}
    \frac{1}{n}\lg\frac{1}{n} \leq \frac{1}{n}\sum_{i=1}^n p_i \lg p_i = - \frac{1}{n}H(p_1,\ldots,p_n)\,,
\end{equation*}
czyli
\begin{equation*}
    H(p_1,\ldots,p_n) \leq - \lg\frac{1}{n} = H(1/n,1/n,\ldots,1/n)\,,
\end{equation*}
czyli niepewność zawarta w danym schemacie skończonym jest mniejsza lub równa od niepewności
zawartej w analogicznym schemacie, w którym wszystkie wyniki są jednakowo prawdopodobne.

Załóżmy teraz, że mamy dwa niezależne schematy skończone
\begin{equation*}
    A = \mqty(A_1 & A_2 & \cdots & A_n \\ p_1 & p_2 & \cdots & p_n)\,,\quad B = \mqty(B_1 & B_2 & \cdots & B_m \\ q_1 & q_2 & \cdots & q_m)
\end{equation*}
takie, że dla każdej pary zdarzeń \(A_i, B_j\) prawdopodobieństwo wystąpienia zdarzenia \(A_iB_j\)
wynosi \(p_iq_j\). Zbiór zdarzeń \(A_iB_j\) z prawdopodobieństwami \(r_{ij} = p_i q_j\) reprezentuje
nowy schemat skończony \(AB\). Wówczas
\begin{equation*}
    \begin{split}
        -H(AB) &= \sum_{i=1}^n\sum_{j=1}^m r_{ij}\lg r_{ij} = \sum_{i=1}^n\sum_{j=1}^m p_iq_j\left(\lg p_i + \lg q_j\right) \\
        &= \sum_{i=1}^n p_i\lg p_i + \sum_{j=1}^m q_j \lg q_j = -H(A) - H(B)\,,  
    \end{split}
\end{equation*}
skąd
\begin{equation*}
    H(AB) = H(A) + H(B)\,.
\end{equation*}
Rozważmy teraz przypadek gdy schematy \(A\), \(B\) są zależne. Przez \(q_{ij}\) oznaczmy
prawdopodobieństwo zajścia zdarzenia \(B_j\) pod warunkiem zdarzenia \(A_i\) tj. \(q_{ij} =
p(B_j\mid A_i)\). Schemat \(AB\) jest teraz opisany prawdopodobieństwami \(r_{ij} = p_i q_{ij}\)
zatem
\begin{equation*}
    -H(AB) = \sum_{i=1}^n\sum_{j=1}^m p_i q_{ij} \left(\lg p_i + \lg q_{ij}\right) = - H(A) + \sum_{i=1}^n p_i \sum_{j=1}^m q_{ij} \lg q_{ij}
\end{equation*}
gdyż \(\sum_j q_{ij} = 1\) (prawdopodobieństwo zajścia dowolnego zdarzenia z B pod warunkiem
wystąpienia zdarzenia \(A_i\) wynosi 1), natomiast wielkość \(-\sum_{j=1}^m q_{ij} \lg q_{ij}\) jest
warunkową entropią schematu \(B\) pod warunkiem zajścia zdarzenia \(A_i\), co oznaczymy jako
\(H(B\mid A=A_i)\)
\begin{equation*}
    H(AB) = H(A) + \sum_{i=1}^n p_i H(B\mid A=A_i)\,.
\end{equation*}
Ostatni człon jest w takim razie wartością oczekiwaną wielkości \(H(B)\) w schemacie \(A\), co
oznaczymy jako \(H(B \mid A)\). Mamy w takim razie
\begin{equation*}
    H(AB) = H(A) + H(B \mid A)\,.
\end{equation*}
Z nierówności Jensena można dodatkowo pokazać, że zachodzi \(H(B\mid A) \leq H(B)\).

\subsubsection{Entropia względna}
Dla dwóch ciągłych rozkładów prawdopodobieństwa \(p(\mathbf{x})\), \(q(\mathbf{x})\) definiujemy ich
entropię względną (nazywaną również \textit{Kullback-Leibler (KL) divergence}) jako
\begin{equation*}
    \mathbb{D}_\text{KL}(p , q) = \int\limits_{\mathbb{R}^n} p(\mathbf{x})\log\frac{p(\mathbf{x})}{q(\mathbf{x})} \dd[n]{\mathbf{x}}\,,
\end{equation*}
która określa podobieństwo między dwoma rozkładami prawdopodobieństwa tj. dla ustalonego rozkładu
\(p\) dla wszystkich \(q\) zachodzi \(\mathbb{D}_\text{KL}(p,q) \geq 0\), przy czym równość zachodzi
iff \(p = q\) (ponownie nierówność Jensena).

Rozważmy teraz rozkład łączny \(p(\mathbf{x}, \mathbf{y})\). Jeśli zmienne losowe \(\mathbf{x},
\mathbf{y}\) są niezależne to \(p(\mathbf{x}, \mathbf{y}) = p(\mathbf{x}) p(\mathbf{y})\). Jeśli
zmienne nie są niezależne to możemy określić stopień ich zależności właśnie poprzez entropię
względną między rozkładem łącznym \(p(\mathbf{x}, \mathbf{y})\), a rozkładem faktoryzowanym
\(p(\mathbf{x})p(\mathbf{y})\). Wielkość taką nazywamy informacją wzajemną (z ang/ \textit{mutual
information})
\begin{equation*}
    \mathbb{I}(\mathbf{x} , \mathbf{y}) = \mathbb{D}_\text{KL}(p(\mathbf{x}, \mathbf{y}), p(\mathbf{x})p(\mathbf{y})) = \int\limits_{\mathbb{R}^n\times\mathbb{R}^m} p(\mathbf{x}, \mathbf{y})\log\left\{\frac{p(\mathbf{x}, \mathbf{y})}{p(\mathbf{x})p(\mathbf{y})}\right\} \dd[n]{\mathbf{x}} \dd[m]{\mathbf{y}}\,.
\end{equation*}

\subsection{Wnioskowanie statystyczne}

Modelem statystycznym nazwiemy parę \((\chi, \mathcal{P})\), gdzie \(\mathcal{P}\) jest rodziną
rozkładów prawdopodobieństwa w zbiorze \(\chi\), przy czym będziemy zakładać \(\chi = \mathbb{R}^n\)
\begin{equation*}
    \mathcal{P} := \left\{p(\mathbf{x} \mid \theta) \mid \theta \in \Theta \right\}\,,
\end{equation*}
gdzie \(\Theta\) jest zbiorem parametrów modelu \(\mathcal{P}\). Prostą próbą losową w modelu
\(\mathcal{P}\) nazwiemy ciąg niezależnych zmiennych losowych \(X_1, \ldots, X_n\) o wartościach w
\(\mathbb{R}^n\) i pochodzących z tego samego rozkładu \(p(\mathbf{x} \mid \theta) \in \mathcal{P}\)
(w angielskiej terminologii taki ciąg zmiennych losowych nazwiemy \textit{i.i.d.} tj.
\textit{independent and identically distributed}). Statystyką z kolei nazwiemy zmienną losową \(T\)
będącą funkcją prostej próby losowej tj. \(T = T(X_1, \ldots, X_n)\). Być może najważniejszym
przykładem statystyki jest średnia oznaczana jako \(\overline{X}\)
\begin{equation*}
    \overline{X}(X_1, \ldots, X_n) := \frac{X_1 + \ldots + X_n}{n}\,.
\end{equation*}
Wartość oczekiwana statystyki średniej \(\overline{X}(X_1, \ldots, X_n)\) dla \(X_i\) z rozkładu \(X
\sim \mathcal{D}\) o gęstości \(p\) wynosi
\begin{equation*}
    \mathbb{E}[\overline{X}] = \int\cdots\int\frac{1}{n}\left(\sum_{i=1}^nX_i\right)p(X_1)\cdots p(X_n) \dd{X_1}\ldots\dd{X_n} = \mathbb{E}[X]\,.
\end{equation*}
Wariancja statystyki średniej wynosi z kolei
\begin{equation*}
    \begin{split}
        &\text{Var}[\overline{X}] = \mathbb{E}[\overline{X}^2] - \mathbb{E}[\overline{X}]^2 \\
        &= \int\cdots\int\frac{1}{n^2}\left(\sum_{i=1}^n X_i^2 + \underbrace{\sum_{i\neq j}X_iX_j}_{n(n-1)}\right) p(X_1)\cdots p(X_n)\dd{X_1}\ldots\dd{X_n} - \mathbb{E}[X]^2\\
        &= \frac{1}{n}\mathbb{E}[X^2] + \frac{n(n-1)}{n^2}\mathbb{E}[X]^2 - \mathbb{E}[X]^2 = \frac{1}{n}\left[\mathbb{E}[X^2] - \mathbb{E}[X]^2\right] = \frac{1}{n}\text{Var}[X]\,.
    \end{split}
\end{equation*}

\subsection{Twierdzenie Gliwenki--Cantelliego}
Dystrybuantą empiryczną nazywa się funkcję
\begin{equation*}
    \hat{F}(x) = \frac{1}{N}\#\left\{i \in \{1,\ldots,N\} \mid x_i \leq x \right\}\,,
\end{equation*}
gdzie \(\{x_1,\ldots,x_N\}\) jest realizacją prostej próby losowej. Twierdzenie
Gliwenki--Cantelliego stwierdza, iż jeśli \(F(x)\) jest dystrybuantą pewnego rozkładu
prawdopodobieństwa to
\begin{equation*}
    \sup_{x\in\mathbb{R}} |\hat{F}_n(x) - F(x)| \to 0\,,\text{przy \(n\to\infty\).}
\end{equation*}

\subsection{Silne prawo wielkich liczb}

Niech \((X_n)\) będzie ciągiem zmiennych losowych i.i.d. z pewnego rozkładu \(X \sim \mathcal{D}\).
Przez \((\overline{X}_n)\) oznaczmy ciąg średnich częściowych tj.
\begin{equation*}
    \overline{X}_n = \frac{1}{n}\sum_{i=1}^n X_i\,.
\end{equation*}
Wówczas zachodzi silne prawo wielkich liczb
\begin{equation*}
    P\left(\lim_{n\to\infty} \overline{X}_n = \mathbb{E}[X]\right) = 1\,,
\end{equation*}
czyli średnia próbek zbiega do wartości oczekiwanej z prawdopodobieństwem 1.

Silne prawo wielkich liczb daje nam potężne narzędzie do szacowania wartości oczekiwanych, gdyż
możemy je przybliżać średnią z dużej liczby próbek losowych, a dokładność tego przybliżenia zależy
jedynie od liczby próbek i wariancji \(X\). Jeśli \(X\) jest zmienną wielowymiarową to dokładność
przybliżenia nie zależy wprost od liczby wymiarów i unikamy tzw. \textit{curse of dimensionality}.

\subsection{Centralne Twierdzenie Graniczne}

Niech \((X_n)\) będzie ciągiem \(k\)--wymiarowych zmiennych losowych i.i.d. z dowolnego rozkładu \(X
\sim \mathcal{D}\) o wartości oczekiwanej \(\boldsymbol{\mu} = \mathbb{E}[\mathbf{x}]\) i
odwracalnej macierzy kowariancji \(\oper{\Sigma}\). Oznaczając przez \((\overline{X}_n)\) ciąg
średnich częściowych ciągu \((X_n)\) zachodzi
\begin{equation*}
    \sqrt{n}\left(\overline{X}_n - \boldsymbol{\mu}\right) \to Z \sim \mathcal{N}(\mathbf{0}, \oper{\Sigma})\,.
\end{equation*}

Oznacza to, iż dla ciągu \(X_1,\ldots,X_n\) zmiennych losowych i.i.d. z praktycznie dowolnego
rozkładu \(X\sim\mathcal{D}\) dla odpowiednio dużych \(n\) średnią z próbek możemy traktować jako
zmienną losową o rozkładzie normalnym \(\mathcal{N}(\boldsymbol{\mu}, n^{-1/2}\oper{\Sigma})\).


\subsection{Estymatory punktowe MLE i MAP}

Rozważamy model statystyczny \(\mathcal{P} = \left\{p(\mathbf{x} \mid \theta) \mid \theta \in
\Theta\right\}\). Estymatorem parametru \(\theta\) nazwiemy statystykę
\(\hat{\theta}(X_1,\ldots,X_n)\) służącą do oszacowania wartości tego parametru. Wartość tej
statystki dla konkretnej realizacji prostej próby losowej
\(\hat{\theta}(\mathbf{x}_1,\ldots,\mathbf{x}_n)\) nazwiemy estymatą parametru \(\theta\). Dodatkowo
definiujemy obciążenie (z ang. \textit{bias}) estymatora jako wielkość
\begin{equation*}
    \mathbb{B}[\hat{\theta}] := \mathbb{E}[\hat{\theta}] - \theta\,.
\end{equation*}

Zasadniczo będą nas interesować dwa rodzaje estymat: MLE i MAP. W przypadku estymaty MLE (z ang.
\textit{Maximum Likelihood Estimate}) definiujemy funkcję wiarygodności (\textit{likelihood}) dla
modelu \(\mathcal{P} = \left\{p(\mathbf{x} \mid \theta) \mid \theta \in \Theta\right\}\) i
realizacji prostej próby losowej (którą nazwiemy również danymi lub obserwacjami)
\(D=(\mathbf{x}_1,\ldots,\mathbf{x}_n)\) jako
\begin{equation*}
    p(D \mid \theta) = \prod_{i=1}^n p(\mathbf{x}_i \mid \theta)\,.
\end{equation*}
Estymatą MLE nazywamy taką wartość parametru \(\theta_\text{MLE} \in \Theta\), że
\begin{equation*}
    p(D \mid \theta_\text{MLE}) = \max_{\theta \in \Theta} p(D \mid \theta)\,.
\end{equation*}
Ponieważ znajdywanie maksimum funkcji będącej iloczynem nie jest zadaniem przyjemnym (chociażby
obliczanie pochodnych iloczynu funkcji jest trudniejsze od sumy), więc wprowadzamy zanegowaną
logarytmiczną funkcję wiarygodności
\begin{equation*}
    \ell(D \mid \theta) = -\log p(D \mid \theta) = -\sum_{i=1}^n \log p(\mathbf{x}_i \mid \theta)\,,
\end{equation*}
wówczas ze względu na fakt, iż funkcja \(\log x\) jest ściśle rosnąca estymatę MLE możemy
równoważnie wyznaczyć jako
\begin{equation*}
    \ell(D \mid \theta_\text{MLE}) = \min_{\theta \in \Theta} \ell(D \mid \theta)\,.
\end{equation*}
Funkcję \(\ell\) będziemy również nazywać funkcją kosztu.

W przypadku estymaty MAP (z ang. \textit{Maximum a posteriori estimate}) wprowadzamy gęstość
rozkładu a posteriori jako
\begin{equation*}
    p(\theta \mid D) = \frac{1}{Z}p(D \mid \theta)\pi(\theta)\,,
\end{equation*}
gdzie \(Z\) jest stałą wynikającą z warunku unormowania, a \(\pi(\theta)\) to gęstość
prawdopodobieństwa opisująca rozkład a priori parametru \(\theta\). Estymatą MAP nazywamy taką
wartość parametru \(\theta_\text{MAP} \in \Theta\), że
\begin{equation*}
    p(\theta_\text{MAP} \mid D) = \max_{\theta \in \Theta} p(\theta \mid D)\,.
\end{equation*}
Zauważmy przy tym iż liczba \(Z\) nie jest nam potrzebna, gdyż wystarczy zmaksymalizować licznik tj.
\begin{equation*}
    \theta_\text{MAP} = \arg\max_{\theta \in \Theta} p(D \mid \theta)\pi(\theta)\,.
\end{equation*}

\linesep
\newpage

\section{Probabilistyczne uczenie maszynowe}

\subsection{Wnioskowanie Bayesowskie}

Zajmiemy się teraz wnioskowaniem opartym na twierdzeniu Bayesa. Rozpatrujemy model statystyczny
\(\mathcal{P} = \left\{p(\mathbf{x} \mid \theta) \mid \theta \in \Theta\right\}\). Załóżmy, iż mamy
obserwacje \(D = (\mathbf{x}_1, \ldots, \mathbf{x}_n)\), wówczas twierdzenie Bayesa możemy zapisać
jako
\begin{equation*}
    p(\theta \mid D) = \frac{p(D \mid \theta)\pi(\theta)}{p_D(D)} = \frac{p(D \mid \theta)\pi(\theta)}{\int\limits_\Theta p(D \mid \theta)\pi(\theta) \dd{\theta}}\,,
\end{equation*}
gdzie \(p(\theta \mid D)\) nazywamy rozkładem a posteriori (posteriorem), \(p(D \mid \theta)\) --
wiarygodnością (likelihood), a \(\pi(\theta)\) -- rozkładem a priori (priorem).

Całe wnioskowanie Bayesowskie opiera się na wyznaczeniu rozkładu a posteriori, który wyraża całą
naszą wiedzę o estymowanym parametrze \(\theta\). Na podstawie tego rozkładu możemy wyznaczyć
estymatę punktową MAP maksymalizującą gęstość prawdopodobieństwa a posteriori, jak również
niepewność związaną z wyznaczeniem tej estymaty np. poprzez wyznaczenie przedziału wiarygodności
\(C_{1-\alpha}(\theta \mid D) = [\theta_l ; \theta_u]\) takiego, że
\begin{equation*}
    P(\theta \in [\theta_l ; \theta_u] \mid D) = 1 - \alpha\,,
\end{equation*}
dla ustalonego \(0 < \alpha < 1\). Możemy również skonstruować rozkład predykcyjny (z ang.
\textit{posterior predictive distribution}) określający prawdopodobieństwo zaobserwowania nowej
obserwacji \(\mathbf{x}\)
\begin{equation*}
    p(\mathbf{x} \mid D) = \int\limits_\Theta p(\mathbf{x} \mid \theta) p(\theta \mid D) \dd{\theta}\,.
\end{equation*}
Znając rozkład a posteriori estymowanego parametru \(\theta\) możemy nie tylko wyznaczyć estymaty
punktowe, wartości oczekiwane i przedziały wiarygodności, ale również znaleźć estymator Bayesa (z
ang. \textit{Bayes estimator}), który minimalizuje wartość oczekiwaną pewnej funkcji kosztu (z ang.
\textit{loss/cost function}) \(L(\theta, \hat{\theta})\) po wszystkich estymatorach \(\hat{\theta}\)
\begin{equation*}
    \theta_\text{Bayes} = \arg\min_{\hat{\theta}} \int\limits_{\Theta} L(\theta, \hat{\theta}) p(\theta \mid D) \dd{\theta}\,.
\end{equation*}
Całkę w powyższym wzorze nazywa się również funkcją ryzyka (z ang. \textit{risk function})
\(R(\hat{\theta})\), która określa oczekiwaną stratę spowodowaną wykorzystaniem danego estymatora
parametru \(\theta\). W przypadku gdy funkcja kosztu ma postać błędu kwadratowego (L2)
\begin{equation*}
    L(\theta,\hat{\theta}) = (\theta - \hat{\theta})^2
\end{equation*}
funkcję ryzyka możemy zapisać jako
\begin{equation*}
    \begin{split}
        R(\hat{\theta}) &= \int\limits_\Theta \theta^2 p(\theta \mid D) \dd{\theta} - 2 \hat{\theta}\int\limits_\Theta \theta p(\theta \mid D) \dd{\theta} + \hat{\theta}^2 \\
        &= \text{Var}[\theta \mid D] + \mathbb{E}[\theta \mid D]^2 - 2 \hat{\theta}\mathbb{E}[\theta \mid D] + \hat{\theta}^2\\
        &= \text{Var}[\theta \mid D] + \left(\mathbb{E}[\theta \mid D] - \hat{\theta}\right)^2\,.
    \end{split}
\end{equation*}

\subsection{Bayesowski wybór modeli}

Załóżmy, iż mamy rodzinę \(\mathcal{M}\) modeli statystycznych (może to być zbiór dyskretny lub
zbiór modeli indeksowanych ciągłym, wielowymiarowym parametrem \(\boldsymbol{\lambda}\)). Naszym
zadaniem jest wybór najbardziej prawdopodobnego modelu dla danych \(D\). Możemy na to zadanie
patrzeć jako zadanie z teorii decyzji: dla danej funkcji kosztu \(L(M, M^*)\) i rozkładu a
posteriori nad modelami \(p(M \mid D)\) chcemy wybrać model, który minimalizuje ryzyko
\(\mathbb{E}[L(M,M^*)]\). Jeśli jako koszt wybierzemy tzw. \textit{0--1 loss} tj.
\begin{equation*}
    L(M, M^*) = \begin{cases}
        0&\,, \text{jeśli \(M = M^*\)} \\
        1&\,, \text{w.p.p.}
    \end{cases}
\end{equation*}
to
\begin{equation*}
    \mathbb{E}[L(M, M^*)] = 1 - p(M^* \mid D)
\end{equation*}
i wybieramy model \(M\) o największym prawdopodobieństwie (estymata MAP). Pozostaje tylko
wyznaczenie \(p(M \mid D)\)
\begin{equation*}
    p(M \mid D) = \frac{p(D \mid M)\pi(M)}{\sum_{M\in\mathcal{M}} p(D \mid M)\pi(M)}\,.
\end{equation*}
Jeśli jako prior przyjmiemy rozkład jednostajny \(\pi(M) = |\mathcal{M}|^{-1}\) to estymata MAP
sprowadza się do MLE czyli szukamy modelu
\begin{equation*}
    M^* = \arg\max_{M\in\mathcal{M}} p(D \mid M)\,.
\end{equation*}
Jeśli przez \(\theta_M\) oznaczymy parametry modelu \(M\) to 
\begin{equation*}
    p(D \mid M) = \int\limits_{\Theta_M} p(D \mid \theta_M) \pi(\theta_M) \dd{\theta}_M\,.
\end{equation*}
Powyższą wielkość nazywamy wiarygodnością brzegową (z ang. \textit{marginal likelihood}) lub
\textit{model evidence}.

\subsection{Estymator jądrowy gęstości (KDE)}

Załóżmy, że mamy zbiór obserwacji iid \(\{\mathbf{x}_1, \ldots, \mathbf{x}_m\}\) taki, że
\(\mathbf{x}_i \sim \mathcal{D}\) dla pewnego \(n\)--wymiarowego ciągłego rozkładu
prawdopodobieństwa \(\mathcal{D}\) z nieznaną gęstością prawdopodobieństwa \(p(\mathbf{x})\). Chcemy
znaleźć estymator \(\hat{p}(\mathbf{x})\) tej funkcji. Estymatorem jądrowym gęstości funkcji \(p\)
(z ang. \textit{kernel density estimator}) nazywamy funkcję
\begin{equation*}
    \hat{p}(\mathbf{x}) := \frac{1}{mh^n}\sum_{i=1}^m K\left(\frac{\mathbf{x} - \mathbf{x}_i}{h}\right)\,,
\end{equation*}
gdzie \(h \in \mathbb{R}\) jest pewnym hiperparametrem zwanym \textit{bandwidth}, a \(K:
\mathbb{R}^n \mapsto [0; \infty)\) to tzw. funkcja jądrowa będąca parzystą funkcją posiadającą w 0
maksimum globalne oraz spełniającą warunek unormowania
\begin{equation*}
    \int\limits_{\mathbb{R}^n} K(\mathbf{x}) \dd[n]{\mathbf{x}}= 1\,.
\end{equation*}
Ze statystycznego punktu widzenia, postać jądra nie ma istotnego znaczenia i wybór funkcji  \(K\)
może być arbitralny, uwzględniający przede wszystkim pożądane własności otrzymanego estymatora, na
przykład klasę jego regularności (ciągłość, różniczkowalność itp.). W przypadku jednowymiarowym jako
funkcję \(K\) przyjmuje się klasyczne postacie gęstości rozkładów probabilistycznych, na przykład
gęstość rozkładu normalnego. W przypadku wielowymiarowym stosuje się tzw. jądro radialne tj. dla
jądra jednowymiarowego \(K\) wielowymiarowe jądro radialne definiujemy jako
\begin{equation*}
    K(\mathbf{x}) = K(\norm{\mathbf{x}})
\end{equation*}
dla pewnej normy (typowo normy euklidesowej) \(\norm{\cdot}\).

\subsection{Modele Gaussowskie}

Jak już wspomnieliśmy w przypadku gdy zmienna losowa ma wielowymiarowy rozkład normalny
\(\mathcal{N}(\boldsymbol{\mu}, \oper{\Sigma})\) wszystkie rozkłady brzegowe i warunkowe są również
rozkładami normalnymi. W szczególnym przypadku gdy zmienne \(k\) i \(n-k\) --wymiarowe
\(\mathbf{x}\) i \(\mathbf{y}\) mają łącznie rozkład normalny
\begin{equation*}
    \mqty[\mathbf{x} \\ \mathbf{y}] \sim \mathcal{N}(\boldsymbol{\mu}, \oper{\Sigma})\,,
\end{equation*}
gdzie
\begin{equation*}
    \boldsymbol{\mu} = \mqty[\boldsymbol{\mu}_\mathbf{x} \\ \boldsymbol{\mu}_\mathbf{y}]\,,\quad \oper{\Sigma} = \mqty[\oper{\Sigma}_{\mathbf{xx}} & \oper{\Sigma}_{\mathbf{xy}} \\ \oper{\Sigma}_{\mathbf{yx}} & \oper{\Sigma}_\mathbf{yy}]
\end{equation*}
można pokazać iż
\begin{equation*}
    \mathbf{x} \mid \mathbf{y} \sim \mathcal{N}(\boldsymbol{\mu}_{\mathbf{x}|\mathbf{y}}, \oper{\Sigma}_{\mathbf{x}|\mathbf{y}})\,,\quad \mathbf{y} \sim \mathcal{N}(\boldsymbol{\mu}_\mathbf{y}, \oper{\Sigma}_\mathbf{yy})\,,
\end{equation*}
gdzie
\begin{equation*}
    \begin{split}
        &\boldsymbol{\mu}_{\mathbf{x}|\mathbf{y}} = \boldsymbol{\mu}_\mathbf{x} + \oper{\Sigma}_\mathbf{xy}\oper{\Sigma}_\mathbf{yy}^{-1}(\mathbf{y} - \boldsymbol{\mu}_\mathbf{y})\\
        &\oper{\Sigma}_{\mathbf{x}|\mathbf{y}} = \oper{\Sigma}_\mathbf{xx} - \oper{\Sigma}_\mathbf{xy}\oper{\Sigma}_\mathbf{yy}^{-1}\oper{\Sigma}_\mathbf{yx}
    \end{split}\quad.
\end{equation*}

\subsection{Liniowe modele Gaussowskie}

Powyższe własności rozkładów łącznych pozwalają jawnie wnioskować w tzw. liniowych modelach
Gaussowskich (z ang. \textit{Linear Gaussian Models}). Załóżmy, iż nasze obserwacje są modelowane
przez \(n\)--wymiarową zmienną losową \(\mathbf{y}\) o rozkładzie normalnym z estymowanym parametrem
\(\mathbf{x}\) i znanymi parametrami \(\oper{A}, \mathbf{b}, \oper{\Sigma}_\mathbf{y}\) tak, że
wiarygodność ma postać
\begin{equation*}
    \mathbf{y}\mid\mathbf{x} \sim \mathcal{N}(\oper{A}\mathbf{x} + \mathbf{b}, \oper{\Sigma}_\mathbf{y})\,,
\end{equation*}
gdzie \(\oper{A}\) jest macierzą wymiaru \(n\times k\). Jako prior na parametr \(\mathbf{x}\)
przyjmiemy również rozkład normalny o pewnych zadanych parametrach \(\boldsymbol{\mu}_\mathbf{x},
\oper{\Sigma}_\mathbf{x}\) (taki wybór rozkładu a priori nazywamy rozkładem sprzężonym do
wiarygodności)
\begin{equation*}
    \mathbf{x} \sim \mathcal{N}(\boldsymbol{\mu}_\mathbf{x}, \oper{\Sigma}_\mathbf{x})\,.
\end{equation*}
Wówczas łatwo pokazać, iż rozkład a posteriori jest rozkładem normalnym
\begin{equation*}
    \mathbf{x} \mid \mathbf{y} \sim \mathcal{N}(\boldsymbol{\mu}_{\mathbf{x}|\mathbf{y}}, \oper{\Sigma}_{\mathbf{x}|\mathbf{y}})
\end{equation*}
z parametrami
\begin{equation*}
    \begin{split}
        &\oper{\Sigma}_{\mathbf{x}|\mathbf{y}} = \left[\oper{\Sigma}_\mathbf{x}^{-1} + \oper{A}^\top\oper{\Sigma}_\mathbf{y}^{-1}\oper{A}\right]^{-1}\\
        &\boldsymbol{\mu}_{\mathbf{x}|\mathbf{y}} = \oper{\Sigma}_{\mathbf{x}|\mathbf{y}}\left[\oper{A}^\top\oper{\Sigma}_\mathbf{y}^{-1}(\mathbf{y} - \mathbf{b}) + \oper{\Sigma}_\mathbf{x}^{-1}\boldsymbol{\mu}_\mathbf{x}\right]
    \end{split}\quad.
\end{equation*}

Załóżmy teraz, iż mamy ciąg obserwacji \((\mathbf{y}_1, \ldots, \mathbf{y}_m)\). Wnioskowanie
Bayesowskie możemy wówczas stosować iteracyjnie tzn. na początku dla 0 obserwacji rozkład
estymowanego parametru jest opisany przez prior \(\mathcal{N}(\boldsymbol{\mu}_0,
\oper{\Sigma}_0)\). Po zaobserwowaniu jednego \(\mathbf{y}_1\) aktualizujemy nasze przekonania co do
parametru \(\mathbf{x}\) zgodnie z powyższym wzorem i otrzymujemy rozkład normalny o parametrach
\begin{equation*}
    \begin{split}
        &\oper{\Sigma}_1 = \left[\oper{\Sigma}_0^{-1} + \oper{A}^\top\oper{\Sigma}_\mathbf{y}^{-1}\oper{A}\right]^{-1}\\
        &\boldsymbol{\mu}_1 = \oper{\Sigma}_1\left[\oper{A}^\top\oper{\Sigma}_\mathbf{y}^{-1}(\mathbf{y}_1 - \mathbf{b}) + \oper{\Sigma}_0^{-1}\boldsymbol{\mu}_0\right]
    \end{split}\quad.
\end{equation*}
Po zaobserwowaniu kolejnego \(\mathbf{y}_2\) ponownie wykorzystujemy powyższe wzory ale jako prior
wykorzystując rozkład w poprzedniej iteracji. W ogólności możemy zapisać wzór rekurencyjny na
\(m+1\) rozkład jako
\begin{equation*}
    \begin{split}
        &\oper{\Sigma}_{m+1} = \left[\oper{\Sigma}_m^{-1} + \oper{A}^\top\oper{\Sigma}_\mathbf{y}^{-1}\oper{A}\right]^{-1}\\
        &\boldsymbol{\mu}_{m+1} = \oper{\Sigma}_{m+1}\left[\oper{A}^\top\oper{\Sigma}_\mathbf{y}^{-1}(\mathbf{y}_{m+1} - \mathbf{b}) + \oper{\Sigma}_m^{-1}\boldsymbol{\mu}_m\right]
    \end{split}\quad,
\end{equation*}
skąd możemy od razu podać wzór na parametry \(m\)--tego rozkładu
\begin{equation*}
    \begin{split}
        &\oper{\Sigma}_{m} = \left[\oper{\Sigma}_0^{-1} + m\oper{A}^\top\oper{\Sigma}_\mathbf{y}^{-1}\oper{A}\right]^{-1}\\
        &\boldsymbol{\mu}_{m} = \oper{\Sigma}_{m}\left[\oper{A}^\top\oper{\Sigma}_\mathbf{y}^{-1}\left(\sum_{i=1}^{m}\mathbf{y}_{i} - m\mathbf{b}\right) + \oper{\Sigma}_0^{-1}\boldsymbol{\mu}_0\right]
    \end{split}\quad.
\end{equation*}
Taki sam wynik można by uzyskać rozpatrując łączny rozkład a posteriori dla obserwacji \(D =
(\mathbf{y}_1, \ldots, \mathbf{y}_m)\) tj.
\begin{equation*}
    \begin{split}
        &p(\mathbf{x} \mid D) \cong \pi(\mathbf{x})\prod_{i=1}^m p(\mathbf{y}_i \mid \mathbf{x})\cong \\
        &\exp\left\{-\frac{1}{2}\left[(\mathbf{x}-\boldsymbol{\mu}_0)^\top\oper{\Sigma}_0^{-1}(\mathbf{x}-\boldsymbol{\mu}_0) + \sum_{i=1}^m(\mathbf{y}_i - \oper{A}\mathbf{x}-\mathbf{b})^\top\oper{\Sigma}_\mathbf{y}^{-1}(\mathbf{y}_i - \oper{A}\mathbf{x}-\mathbf{b})\right]\right\}
    \end{split}\,.
\end{equation*}

\subsection{Regresja liniowa}

Załóżmy, iż modelujemy obserwacje postaci \((y,\mathbf{x})\) gdzie \(y\) to skalar zwany zmienną
objaśnianą, którego wartość obserwujemy, a \(\mathbf{x}\) to wektor zmiennych objaśniających, który
kontrolujemy tj. zakładamy, iż wektor \(\mathbf{x}\) dla danego pomiaru \(y\) znamy dokładnie.
Dodatkowo zakładamy, iż \(y\) zależy liniowo od \(\mathbf{x}\) tj.
\begin{equation*}
    y = \mathbf{w}^\top\mathbf{x} + \epsilon\,,
\end{equation*}
gdzie \(\epsilon \sim \mathcal{N}(0, \sigma^2)\) dla znanego \(\sigma\) jest tzw. błędem losowym, a
\(\mathbf{w}\) jest estymowanym przez nas parametrem. Możemy zatem zapisać
\begin{equation*}
    y \mid \mathbf{w} \sim \mathcal{N}(\mathbf{w}^\top\mathbf{x}, \sigma^2)\,.
\end{equation*}
Powiedzmy, iż zaobserwowaliśmy ciąg obserwacji \(D = (y_1, \ldots, y_m)\) dla zadanych (lub
dokładnie znanych) przez nas \((\mathbf{x}_1,\ldots,\mathbf{x}_m)\). Wiarygodność ma zatem postać
\begin{equation*}
    p(D \mid \mathbf{w}) \cong \prod_{i=1}^m \exp\left\{-\frac{1}{2\sigma^2}\left(y_i - \mathbf{w}^\top\mathbf{x}_i\right)^2\right\}\,.
\end{equation*}
W przypadku regresji liniowej zamiast pełnego wnioskowania Bayesowskiego o parametrze \(\mathbf{w}\)
często stosuje się prostsze podejście polegające na znalezieniu estymaty punktowej MLE. Zanegowana
logarytmiczna funkcja wiarygodności ma postać
\begin{equation*}
    \ell(D \mid \mathbf{w}) = \frac{1}{2\sigma^2}\sum_{i=1}^m(y_i - \mathbf{w}^\top\mathbf{x}_i)^2 + \text{const.}
\end{equation*}
Człon stały możemy oczywiście pominąć i zapisać
\begin{equation*}
    \ell(D \mid \mathbf{w}) \cong \sum_{i=1}^m(y_i - \mathbf{w}^\top\mathbf{x}_i)^2 = \left(\mathbf{y} - \oper{X}\mathbf{w}\right)^\top\left(\mathbf{y} - \oper{X}\mathbf{w}\right)\,,
\end{equation*}
gdzie
\begin{equation*}
    \mathbf{y} = \mqty[y_1 \\ \vdots \\ y_m]\,,\quad \oper{X} = \mqty[\mathbf{x}_1^\top \\ \vdots \\ \mathbf{x}_m^\top]\,.
\end{equation*}
Ponieważ otrzymana funkcja \(\ell\) ma postać formy kwadratowej, więc problem optymalizacyjny
polegający na znalezieniu minimum \(\ell\) nazywa się metodą najmniejszych kwadratów (z ang.
\textit{OLS -- Ordinary Least Squares}). Aby wyznaczyć estymatę \(\mathbf{w}_\text{MLE}\) musimy
rozwiązać równanie
\begin{equation*}
    \pdv{\ell}{\mathbf{w}} = \pdv{~}{\mathbf{w}}\left[\mathbf{y}^\top\mathbf{y} + \mathbf{w}^\top\oper{X}^\top\oper{X}\mathbf{w} - 2\mathbf{y}^\top\oper{X}\mathbf{w}\right] = \mathbf{0}\,,
\end{equation*}
skąd
\begin{equation*}
    2\oper{X}^\top\oper{X}\mathbf{w} - 2\oper{X}^\top\mathbf{y} = \mathbf{0}\,,
\end{equation*}
zatem
\begin{equation*}
    \mathbf{w}_\text{MLE} = (\oper{X}^\top\oper{X})^{-1}\oper{X}^\top\mathbf{y}\,.
\end{equation*}
Pełniejszą informację o parametrze \(\mathbf{w}\) możemy uzyskać rozpatrując rozkład a posteriori
\(p(\mathbf{w} \mid D)\). Jeśli jako prior przyjmiemy rozkład normalny z pewnymi parametrami
\(\boldsymbol{\mu}_0, \oper{\Sigma}_0\) to zauważmy, iż otrzymujemy instancję liniowego modelu
Gaussowskiego
\begin{equation*}
    \begin{split}
        \mathbf{y} \mid \mathbf{w} &\sim \mathcal{N}(\oper{X}\mathbf{w}, \sigma^2\oper{1})\\
        \mathbf{w} &\sim \mathcal{N}(\boldsymbol{\mu}_0, \oper{\Sigma}_0)
    \end{split}\quad,
\end{equation*}
skąd rozkład a posteriori jest rozkładem normalnym
\begin{equation*}
    \mathbf{w} \mid \mathbf{y} \sim \mathcal{N}(\boldsymbol{\mu}_m, \oper{\Sigma}_m)
\end{equation*}
o parametrach
\begin{equation*}
    \begin{split}
        &\oper{\Sigma}_m = \left[\oper{\Sigma}_0^{-1} + \sigma^{-2}\oper{X}^\top\oper{X}\right]^{-1}\\
        &\boldsymbol{\mu}_m = \oper{\Sigma}_m\left[\sigma^{-2}\oper{X}^\top\mathbf{y} + \oper{\Sigma}_0^{-1}\boldsymbol{\mu}_0\right]    
    \end{split}\quad.
\end{equation*}
W powyższych wzorach nazwy parametrów nie są przykładowe: po zaobserwowaniu 0 przykładów rozkład
parametru \(\mathbf{w}\) jest rozkładem a priori \(\mathcal{N}(\boldsymbol{\mu}_0,
\oper{\Sigma}_0)\); po zaobserwowaniu po jednej wartości \(y_i\) w \(m\) zadanych (znanych
dokładnie) punktach \(\mathbf{x}_i\) otrzymujemy rozkład a posteriori
\(\mathcal{N}(\boldsymbol{\mu}_m, \oper{\Sigma}_m)\). Gdybyśmy w każdym z \(m\) punktów
\(\mathbf{x}_i\) dokonywali pomiaru \(y_i\) \(s\)--krotnie to wtedy wykorzystując wzory wyprowadzone
przy iteracyjnym stosowaniu wnioskowania w liniowym modelu Gaussowskim otrzymujemy rozkład normalny
o parametrach
\begin{equation*}
    \begin{split}
        &\oper{\Sigma}_{m;s} = \left[\oper{\Sigma}_0^{-1} + \frac{s}{\sigma^2}\oper{X}^\top\oper{X}\right]^{-1}\\
        &\boldsymbol{\mu}_{m;s} = \oper{\Sigma}_{m;s}\left[\sigma^{-2}\oper{X}^\top\sum_{i=1}^s\mathbf{y}_i + \oper{\Sigma}_0^{-1}\boldsymbol{\mu}_0\right]    
    \end{split}\quad.
\end{equation*} 
Rozkład predykcyjny dla nowej obserwacji \(y\) poczynionej w punkcie \(\mathbf{x}\) jest dany przez
\begin{equation*}
    p(y \mid \mathbf{y}) = \int\limits_{\mathbb{R}^n}p(y\mid\mathbf{w})p(\mathbf{w}\mid\mathbf{y}) \dd[n]\mathbf{w}\,.
\end{equation*}
Nietrudno zauważyć, iż będzie to rozkład normalny o parametrach
\begin{equation*}
    \begin{split}
        \mu_{y|\mathbf{y}} = \mathbb{E}[y\mid\mathbf{y}] &= \int\limits_{\mathbb{R}} y p(y\mid\mathbf{y}) \dd{y} = \int\limits_{\mathbb{R}^n}\dd[n]{\mathbf{w}} p(\mathbf{w}\mid\mathbf{y}) \int\limits_\mathbb{R}\dd{y} yp(y\mid\mathbf{w}) \\
        &= \int\limits_{\mathbb{R}^n}\dd[n]{\mathbf{w}} p(\mathbf{w}\mid\mathbf{y}) \mathbf{x}^\top\mathbf{w} = \mathbf{x}^\top\boldsymbol{\mu}_m\,.
    \end{split}
\end{equation*}
oraz
\begin{equation*}
    \begin{split}
        \sigma_{y|\mathbf{y}}^2 &= \mathbb{E}\left[(y - \mu_{y|\mathbf{y}} )^2 \mid \mathbf{y}\right] = \int\limits_{\mathbb{R}^n}\dd[n]{\mathbf{w}} p(\mathbf{w}\mid\mathbf{y}) \int\limits_\mathbb{R}\dd{y} (y-\mu_{y|\mathbf{y}})^2p(y\mid\mathbf{w})\\
        &= \int\limits_{\mathbb{R}^n}\dd[n]{\mathbf{w}} p(\mathbf{w}\mid\mathbf{y}) \int\limits_\mathbb{R}\dd{y} \left(y^2 + \mu_{y|\mathbf{y}}^2 - 2\mu_{y|\mathbf{y}}y\right)p(y\mid\mathbf{w})\\
        &= \int\limits_{\mathbb{R}^n}\dd[n]{\mathbf{w}} p(\mathbf{w}\mid\mathbf{y}) \left(\sigma^2 + (\mathbf{x}^\top\mathbf{w})^2 + \mu_{y|\mathbf{y}}^2 - 2\mu_{y|\mathbf{y}}\mathbf{x}^\top\mathbf{w}\right)\\
        &=\sigma^2 + \int\limits_{\mathbb{R}^n}\dd[n]{\mathbf{w}} p(\mathbf{w}\mid\mathbf{y}) \left(\mathbf{x}^\top\mathbf{w} - \mathbf{x}^\top\boldsymbol{\mu}_m\right)^2\\
        &=\sigma^2 + \mathbf{x}^\top \mathbb{E}[(\mathbf{w}-\boldsymbol{\mu}_m)(\mathbf{w}-\boldsymbol{\mu}_m)^\top\mid\mathbf{y}]\mathbf{x} = \sigma^2 +\mathbf{x}^\top\oper{\Sigma}_m\mathbf{x}\,.
    \end{split}
\end{equation*}
Powyżej skorzystaliśmy ze znanego faktu, iż dla jednowymiarowej zmiennej losowej zachodzi \(\sigma^2
= \mathbb{E}[(X - \mu_X)^2] = \mathbb{E}[X^2] - \mu_X^2\), skąd \(\mathbb{E}[X^2] = \sigma^2 +
\mu_X^2\). Podsumowując rozkład predykcyjny ma postać
\begin{equation*}
    y \mid \mathbf{y} \sim \mathcal{N}(\mathbf{x}^\top\boldsymbol{\mu}_m, \sigma^2 + \mathbf{x}^\top\oper{\Sigma}_m\mathbf{x})\,.
\end{equation*}

\subsection{Regularyzacja}

Regularyzacją nazywamy proces polegający na wprowadzeniu ad hoc do zagadnienia optymalizacji
dodatkowych członów tak, aby rozwiązanie było regularne (prostsze, nieosobliwe, jednoznaczne ...). W
przypadku funkcji kosztu \(\ell\) najczęściej dodajemy człon penalizujący rozwiązania o dużej normie
estymowanego parametru postaci
\begin{equation*}
    \gamma\norm{\theta}
\end{equation*}
dla pewnej normy \(\norm{\cdot}\) i hiper-parametru \(\gamma\) określającego siłę regularyzacji. W
kontekście Bayesowskim regularyzację można również rozumieć jako pewną niechęć ("tłumienie",
zachowawczość) modelu do zmiany rozkładu a priori estymowanego parametru po pojawieniu się kolejnych
obserwacji.

Przykładowo jeśli w zagadnieniu Bayesowskiej regresji liniowej jako prior przyjmiemy rozkład
normalny
\begin{equation*}
    \mathbf{w} \sim \mathcal{N}(\mathbf{0}, \tau^2\oper{1})
\end{equation*}
to rozkład a posteriori jest rozkładem normalnym o parametrach
\begin{equation*}
    \begin{split}
        &\oper{\Sigma}_m = \sigma^2 \left[\gamma\oper{1} + \oper{X}^\top\oper{X}\right]^{-1}\\
        &\boldsymbol{\mu}_m = \left[\gamma\oper{1} + \oper{X}^\top\oper{X}\right]^{-1}\oper{X}^\top\mathbf{y}
    \end{split}\quad,
\end{equation*}
gdzie \(\gamma = \sigma^2 / \tau^2\) jest hiper-parametrem określającym siłę regularyzacji.
Zauważmy, że im większa jest wartość \(\gamma\) (mniejsza niepewność związana z rozkładem a priori)
tym drugi człon w nawiasie staje się mniej istotny. Taki sam wynik możemy uzyskać metodą OLS jeśli
do funkcji kosztu dodamy człon regularyzujący dla zwykłej normy euklidesowej. Zagadnienie
minimalizacji funkcji kosztu będącej formą kwadratową z dodanym członem regularyzującym nazywamy
również regresją grzbietową.

\subsection{Procesy Gaussowskie}

Jak już wspomnieliśmy macierz kowariancji \(n\)--wymiarowej zmiennej losowej \(\mathbf{x}\) o
wartości oczekiwanej \(\boldsymbol{\mu}\) jest zdefiniowana jako
\begin{equation*}
    \oper{\Sigma} = \mathbb{E}\left[(\mathbf{x} - \boldsymbol{\mu})(\mathbf{x} - \boldsymbol{\mu})^\top\right]\,.
\end{equation*}
Pokazaliśmy również, iż macierz ta jest nieujemnie określona. Dodatkowo pokażemy, iż dla każdej
nieujemnie określonej macierzy symetrycznej \(\oper{K}\) wymiaru \(n\times n\) istnieje
\(n\)--wymiarowa zmienna losowa o wielowymiarowym rozkładzie normalnym dla której \(\oper{K}\) jest
macierzą kowariancji. Istotnie dla każdej nieujemnie określonej macierzy symetrycznej istnieje
macierz \(\oper{L}\) taka, że
\begin{equation*}
    \oper{K} = \oper{L}\oper{L}^\top\,,
\end{equation*}
jest to tzw. dekompozycja Choleskiego. Niech \(\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \oper{1})\),
wówczas zmienna losowa \(\oper{L}\mathbf{z}\) ma rozkład o zerowej wartości oczekiwanej i macierzy
kowariancji
\begin{equation*}
    \mathbb{E}\left[(\oper{L}\mathbf{z})(\oper{L}\mathbf{z})^\top\right] = \mathbb{E}\left[\oper{L}\mathbf{z}\mathbf{z}^\top\oper{L}^\top\right] = \oper{L}\mathbb{E}[\mathbf{z}\mathbf{z}^\top]\oper{L}^\top = \oper{L}\oper{1}\oper{L}^\top = \oper{K}\,.
\end{equation*}
Powyższe własności wskazują, iż macierze kowariancji można w pewnym sensie utożsamiać z nieujemnie
określonymi macierzami symetrycznymi.

Zdefiniujemy teraz funkcję kowariancji \(k: \mathbb{R}^n\times\mathbb{R}^n\mapsto\mathbb{R}\) taką,
że \(\forall m\in\mathbb{N} : \forall X = \{\mathbf{x}_1,\ldots,\mathbf{x}_m\} \subset
\mathbb{R}^n\) macierz
\begin{equation*}
    k(X,X) = \mqty[k(\mathbf{x}_1, \mathbf{x}_1) & k(\mathbf{x}_1, \mathbf{x}_2) & \cdots & k(\mathbf{x}_1, \mathbf{x}_m)\\
    k(\mathbf{x}_2, \mathbf{x}_1) & k(\mathbf{x}_2, \mathbf{x}_2) & \cdots & k(\mathbf{x}_2, \mathbf{x}_m)\\
    \vdots & \vdots & \ddots & \vdots\\
    k(\mathbf{x}_m, \mathbf{x}_1) & k(\mathbf{x}_m, \mathbf{x}_2) & \cdots & k(\mathbf{x}_m, \mathbf{x}_m)\\]
\end{equation*}
jest dodatnio określoną macierzą symetryczną. Funkcję \(k\) nazywamy również jądrem dodatnio
określonym (z ang. \textit{positive definite kernel}) lub jądrem Mercera. Dla dwóch zbiorów punktów
\(X = \{\mathbf{x}_1,\ldots,\mathbf{x}_m\} \subset \mathbb{R}^n\) i \(Y =
\{\mathbf{y}_1,\ldots,\mathbf{y}_s\} \subset \mathbb{R}^n\) i funkcji kowariancji \(k\) wprowadzimy
oznaczenie
\begin{equation*}
    k(X,Y) := \mqty[k(\mathbf{x}_1, \mathbf{y}_1) & k(\mathbf{x}_1, \mathbf{y}_2) & \cdots & k(\mathbf{x}_1, \mathbf{y}_s)\\
    k(\mathbf{x}_2, \mathbf{y}_1) & k(\mathbf{x}_2, \mathbf{y}_2) & \cdots & k(\mathbf{x}_2, \mathbf{y}_s)\\
    \vdots & \vdots & \ddots & \vdots\\
    k(\mathbf{x}_m, \mathbf{y}_1) & k(\mathbf{x}_m, \mathbf{y}_2) & \cdots & k(\mathbf{x}_m, \mathbf{y}_s)\\]\,.
\end{equation*}
Poniżej podajemy kilka przykładów funkcji kowariancji
\begin{itemize}
    \item \textit{Gaussian kernel} dla normy \(\norm{\cdot}\) i hiper-parametrów \(a,l\) (amplituda
    i skala długości)
    \begin{equation*}
        k(\mathbf{x}, \mathbf{y}) = a^2\exp\left\{-\frac{1}{2l^2}\norm{\mathbf{x} - \mathbf{y}}^2\right\}
    \end{equation*}
    
    \item \textit{Periodic kernel} dla normy \(\norm{\cdot}\) i hiper-parametrów \(a, l, p\)
    (amplituda, skala długości, okres zmienności)
    \begin{equation*}
        k(\mathbf{x},\mathbf{y}) = a^2\exp\left\{-\frac{2}{l^2}\sin^2\left(\frac{\pi}{p}\norm{\mathbf{x} - \mathbf{y}}\right)\right\}
    \end{equation*}

    \item \textit{White noise kernel} dla hiper-parametru \(\sigma\)
    \begin{equation*}
        k(\mathbf{x},\mathbf{y}) = \sigma^2 \delta_{\mathbf{x},\mathbf{y}}
    \end{equation*}

    \item \textit{Mat\'ern kernel} dla normy \(\norm{\cdot}\) i hiper-parametrów \(a, l, \nu\)
    (amplituda, skala długości, regularność)
    \begin{equation*}
        k(\mathbf{x},\mathbf{y}) = a^2 \frac{2^{1-\nu}}{\Gamma(\nu)}\left(\frac{\sqrt{2\nu}}{l}\norm{\mathbf{x} - \mathbf{y}}\right)^\nu K_\nu\left(\frac{\sqrt{2\nu}}{l}\norm{\mathbf{x} - \mathbf{y}}\right)\,,
    \end{equation*}
    gdzie \(\Gamma(x)\) to funkcja gamma Eulera, a \(K_\nu(x)\) to zmodyfikowana funkcja Bessela
    2-go rodzaju rzędu \(\nu\).

\end{itemize}
Dodatkowo suma lub iloczyn dwóch funkcji kowariancji oraz złożenie funkcji kowariancji z wielomianem
o nieujemnych współczynnikach jest również funkcją kowariancji.

Procesem Gaussowskim (z ang. \textit{Gaussian Process}) nazywamy rodzinę skalarnych zmiennych
losowych indeksowanych przez punkty \(\mathbf{x} \in \mathbb{R}^n\)
\begin{equation*}
    \mathcal{GP} = \left\{f_\mathbf{x} \mid \mathbf{x} \in \mathbb{R}^n\right\}
\end{equation*}
taką że każdy skończony podzbiór \(\mathcal{GP}\) ma łącznie wielowymiarowy rozkład normalny tj. dla
dowolnego zbioru \(X = \{\mathbf{x}_1, \ldots, \mathbf{x}_m\} \subset \mathbb{R}^n\) zachodzi
\begin{equation*}
    \mqty[f_{\mathbf{x}_1} \\ \vdots \\ f_{\mathbf{x}_m}] \sim \mathcal{N}(\boldsymbol{\mu}_X, \oper{\Sigma}_{X})\,.
\end{equation*}
Zauważmy, iż process Gaussowski możemy jednoznacznie zdefiniować podając przepisy na parametry
\(\boldsymbol{\mu}_X\) i \(\oper{\Sigma}_X\) dla dowolnego zbioru \(X\). W praktyce często
przyjmujemy \(\boldsymbol{\mu}_X = \mathbf{0}\), natomiast przepisem na macierz kowariancji może być
zdefiniowana wyżej funkcja kowariancji \(k(X,X)\) tj.
\begin{equation*}
    \mqty[f_{\mathbf{x}_1} \\ \vdots \\ f_{\mathbf{x}_m}] \sim \mathcal{N}(\mathbf{0}, k(X,X))\,.
\end{equation*}
Process Gaussowski daje nam w praktyce rozkład prawdopodobieństwa nad funkcjami
\(f:\mathbb{R}^n\mapsto\mathbb{R}\), których charakter jest określony przez jądro \(k\) (np. funkcja
gładka dla jądra Gaussowskiego, okresowa dla jądra periodycznego, itp.). Zauważmy, że nie
wnioskujemy tu o parametrach konkretnej rodziny funkcji (jak w przypadku regresji liniowej);
interesuje nas jedynie rozkład predykcyjny. Załóżmy, iż w zadanych (lub dokładnie znanych) przez nas
punktach \(X = \{\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_m\}\) zaobserwowaliśmy wartości pewnej
funkcji, o których zakładamy, iż pochodzą z procesu Gaussowskiego zadanego jądrem \(k\), które
wyraża nasze założenia a priori co do charakteru badanej funkcji
\begin{equation*}
    \mathbf{f}_X = \mqty[f_{\mathbf{x}_1} \\ \vdots \\ f_{\mathbf{x}_m}] \sim \mathcal{N}(\mathbf{0}, k(X,X))\,.
\end{equation*}
Powiedzmy, iż chcemy znać wartości \(\mathbf{f}_Y\) tej funkcji w zadanych punktach \(Y =
\{\mathbf{y}_1,\mathbf{y}_2,\ldots,\mathbf{y}_s\}\). Ponieważ założyliśmy, iż wartości funkcji
pochodzą z procesu Gaussowskiego, więc rozkład łączny \(\mathbf{f}_X\) i \(\mathbf{f}_Y\) jest
rozkładem normalnym
\begin{equation*}
    \mqty[\mathbf{f}_X \\ \mathbf{f}_Y] \sim \mathcal{N}\left(\mathbf{0}, \mqty[k(X,X) & k(X,Y) \\ k(Y,X) & k(Y,Y)]\right)\,.
\end{equation*}
Zauważmy, iż jest to instancja modelu Gaussowskiego, więc rozkład warunkowy \(\mathbf{f}_Y\mid
\mathbf{f}_X\) jest również rozkładem normalnym o parametrach
\begin{equation*}
    \begin{split}
        &\boldsymbol{\mu} = k(Y,X)k^{-1}(X,X)\mathbf{f}_X\\
        &\oper{\Sigma} = k(Y,Y) - k(Y,X)k^{-1}(X,X)k(X,Y)
    \end{split}\quad.
\end{equation*}
Dodatkową niepewność związaną z pomiarem wartości \(\mathbf{f}_X\) możemy uchwycić zmieniając postać
jądra 
\begin{equation*}
    k(\mathbf{x},\mathbf{y}) \leftarrow k(\mathbf{x},\mathbf{y}) + \mathcal{I}_X(\mathbf{x})\sigma^2\delta_{\mathbf{x},\mathbf{y}}\,,
\end{equation*}
gdzie \(\sigma\) jest hiper-parametrem określającym precyzję pomiaru. Oczywiście \(k\) jest dalej
funkcją kowariancji, gdyż takie podstawienie powoduje jedynie dodanie dodatnich członów do pewnych
elementów diagonalnych macierzy kowariancji, więc macierz ta jest nadal symetryczna i dodatnio
określona. Wówczas rozkład predykcyjny ma parametry
\begin{equation*}
    \begin{split}
        &\boldsymbol{\mu} = k(Y,X)\left[k(X,X) + \sigma^2\oper{1}\right]^{-1}\mathbf{f}_X\\
        &\oper{\Sigma} = k(Y,Y) - k(Y,X)\left[k(X,X) + \sigma^2\oper{1}\right]^{-1}k(X,Y)
    \end{split}\quad.
\end{equation*}

Pozostaje jeszcze kwestia początkowej estymacji parametrów funkcji jądrowych dla zbioru danych \(D =
(\mathbf{f}_X, X)\). W łatwy i wydajny sposób możemy znaleźć estymaty MLE tych parametrów. Istotnie
oznaczmy zbiór parametrów jądra przez \(\theta\), wówczas
\begin{equation*}
    D \mid \theta \sim \mathcal{N}(\mathbf{0}, k(X,X; \theta))
\end{equation*}
czyli
\begin{equation*}
    \ell = -\log p(D \mid \theta) = \frac{1}{2}\log\left[\det (k(X,X;\theta)+\sigma^2\oper{1})\right] + \frac{1}{2}\mathbf{f}_X^\top\left[k(X,X;\theta) + \sigma^2\oper{1}\right]^{-1}\mathbf{f}_X\,.
\end{equation*}
Funkcję kosztu \(\ell\) minimalizujemy numerycznie, aby znaleźć estymatę MLE parametrów \(\theta\)
funkcji jądrowej. Następnie parametry rozkładu predykcyjnego nad wartościami w nowych punktach
znajdujemy korzystając ze wzorów dwie linijki wyżej.

\subsection{Wieloklasowa regresja logistyczna}

Załóżmy, iż modelujemy obserwacje postaci \((t,\mathbf{x})\), gdzie \(t \in
\{\tau_1,\tau_2,\ldots,\tau_s\}\) to etykieta określająca przynależność do jednej z \(s\) klas, a
\(\mathbf{x} \in \mathbb{R}^n\) jest znanym (lub zadanym) przez nas dokładnie wektorem cech obiektu
dla których zaobserwowaną klasą jest \(t\). Zakładamy ponadto, iż prawdopodobieństwo przynależności
do klasy \(\tau_j\) (jednej z \(s\) klas) dla wektora cech \(\mathbf{x}\) ma postać tzw. funkcji
softmax
\begin{equation*}
    \pi_j(\mathbf{x}) = \frac{1}{Z(\mathbf{x})}\e^{\mathbf{w}_j^\top\mathbf{x}}\,,
\end{equation*}
gdzie \(\mathbf{w}_j\) są estymowanymi przez nas parametrami. Ze względu na warunek unormowania
musimy mieć
\begin{equation*}
    \sum_{j=1}^s \pi_j = 1\,,
\end{equation*}
skąd stała normalizacyjna \(Z(\mathbf{x})\) ma postać
\begin{equation*}
    Z(\mathbf{x}) = \sum_{j=1}^s \e^{\mathbf{w}_j^\top\mathbf{x}}\,.
\end{equation*}
Uwaga, powyższy wzór nie może być stosowany bezpośrednio do obliczeń numerycznych przez
niestabilność numeryczną (wartość funkcji wykładniczej może być ogromna). W praktyce wartość funkcji
softmax dla argumentu \(\mathbf{z} = (z_1,\ldots,z_n)\) obliczamy jako
\begin{equation*}
    \begin{split}
        &z'_i := z_i - \max_{j=1,..,n} z_j\\
        &\text{softmax}(z_i) = \frac{\e^{z_i'}}{\sum_{j=1}^n \e^{z_j'}}\,.
    \end{split}
\end{equation*}

Rozkład zmiennej losowej \(t\) jest  w takim razie dyskretnym rozkładem wielopunktowym (z ang.
\textit{categorical distribution}) postaci
\begin{equation*}
    t \mid \mathbf{w}_1,\ldots,\mathbf{w}_s \sim \text{Cat}(\pi_1(\mathbf{x}),\ldots,\pi_s(\mathbf{x}))\,.
\end{equation*}
Zauważmy, iż prawdopodobieństwo wylosowania etykiety \(t\) dla parametrów \(\mathbf{w}_j\) możemy
zapisać jako
\begin{equation*}
    p(t \mid \mathbf{w}_1,\ldots,\mathbf{w}_s) = \prod_{j=1}^s \pi_j(\mathbf{x})^{\delta(t,\tau_j)}\,.
\end{equation*}
Powiedzmy, że mamy obserwacje \(D = (t_1,\ldots,t_m)\) dla znanych (lub zadanych) przez nas
dokładnie wektorów cech \((\mathbf{x}_1, \ldots, \mathbf{x}_m)\). Funkcja wiarygodności ma wówczas
postać
\begin{equation*}
    p(D \mid \mathbf{w}_1,\ldots,\mathbf{w}_s) = \prod_{i=1}^m p(t_i \mid \mathbf{w}_1,\ldots,\mathbf{w}_s) = \prod_{i=1}^m \prod_{j=1}^s \pi_j(\mathbf{x}_i)^{\delta(t_i,\tau_j)}\,.
\end{equation*}
Jako prior dla parametrów \(\mathbf{w}_j\) przyjmiemy rozkład normalny z pewnym hiper-parametrem
\(\gamma\)
\begin{equation*}
    \forall j\in \{1,\ldots,s\} : \mathbf{w}_j \sim \mathcal{N}(\mathbf{0}, \gamma^{-1}\oper{1})\,.
\end{equation*}
W przypadku regresji logistycznej ograniczymy się do znalezienia estymaty MAP parametrów
\(\mathbf{w}_j\) tak, aby w przyszłości do nowego wektora cech \(\mathbf{x}\) przyporządkować klasę
o największym prawdopodobieństwie \(\pi_j(\mathbf{x})\). Znalezienie estymaty MAP sprowadza się do
znalezienia minimum zregularyzowanej funkcji kosztu
\begin{equation*}
    \begin{split}
        \ell^*(D \mid \mathbf{w}_1,\ldots,\mathbf{w}_s) &= -\log [p(D \mid \mathbf{w}_1,\ldots,\mathbf{w}_s ) \pi(\mathbf{w}_1, \ldots, \mathbf{w}_s)] \\
        &= - \log \left[\prod_{k=1}^s\e^{-\frac{\gamma}{2}\mathbf{w}_k^\top\mathbf{w}_k}\prod_{i=1}^m \prod_{j=1}^s \pi_j(\mathbf{x}_i)^{\delta(t_i,\tau_j)}\right]\\
        &= \frac{\gamma}{2}\sum_{j=1}^s \mathbf{w}_j^\top\mathbf{w}_j - \sum_{i=1}^m\sum_{j=1}^s \delta(t_i,\tau_j)\log\pi_j(\mathbf{x}_i)\,.
    \end{split}
\end{equation*}
Niestety dla tak zdefiniowanej funkcji kosztu nie można znaleźć wzoru na minimum w postaci
analitycznej, dlatego wykorzystamy numeryczny algorytm optymalizacji zwany spadkiem wzdłuż
gradientu.

\begin{tcolorbox}[title=Algorytm spadku wzdłuż gradientu]
\begin{enumerate}
    \item Wybierz parametry początkowe \(\mathbf{x}_1^{(0)}, \ldots, \mathbf{x}_m^{(0)}\)
    \item Powtarzaj
    \begin{equation*}
        \begin{split}
            &\mathbf{x}_1^{(t+1)} = \mathbf{x}_1^{(t)} - \epsilon_1\pdv{f}{\mathbf{x}_1}\bigg|_{\mathbf{x}_1^{(t)}, \ldots, \mathbf{x}_m^{(t)}}\\
            &\vdots\\
            &\mathbf{x}_m^{(t+1)} = \mathbf{x}_m^{(t)} - \epsilon_m\pdv{f}{\mathbf{x}_m}\bigg|_{\mathbf{x}_1^{(t)}, \ldots, \mathbf{x}_m^{(t)}}
        \end{split}
    \end{equation*}
    gdzie \(\epsilon_1,\ldots,\epsilon_m\) to hiper-parametry zwane stałymi uczącymi (z ang.
    \textit{learning rate}).
\end{enumerate}
\end{tcolorbox}

Zakładając \(\epsilon_1 = \ldots = \epsilon_m = \epsilon\) i wprowadzając
\begin{equation*}
    \oper{X} := \mqty[\mathbf{x}_1^\top \\ \vdots \\ \mathbf{x}_m^\top]\,,\quad \pdv{f}{\oper{X}} := \mqty[\pdv{f}{\mathbf{x}_1}^\top \\ \vdots \\ \pdv{f}{\mathbf{x}_m}^\top]
\end{equation*}
możemy zapisać powyższe równania w kompaktowej formie
\begin{equation*}
    \oper{X}^{(t+1)} = \oper{X}^{(t)} - \epsilon \pdv{f}{\oper{X}}\bigg|_{\oper{X}^{(t)}}\,.
\end{equation*}

Aby zminimalizować numerycznie funkcję kosztu \(\ell^*\) stosując metodę spadku wzdłuż gradientu
musimy obliczyć pochodne funkcji kosztu po parametrach \(\mathbf{w}_j\)
\begin{equation*}
    \pdv{\ell^*}{\mathbf{w}_k} = \gamma\mathbf{w}_k - \sum_{i=1}^m\sum_{j=1}^s \delta(t_i,\tau_j)\pdv{~}{\mathbf{w}_k}\log\pi_j(\mathbf{x}_i)\,,
\end{equation*}
ale
\begin{equation*}
    \begin{split}
        \pdv{~}{\mathbf{w}_k}\log\pi_j(\mathbf{x}_i) &= \frac{1}{\pi_j(\mathbf{x}_i)}\frac{Z(\mathbf{x}_i)\pdv{\e^{\mathbf{x}_i^\top\mathbf{w}_j}}{\mathbf{w}_k} - \e^{\mathbf{x}_i^\top\mathbf{w}_j}\pdv{Z(\mathbf{x}_i)}{\mathbf{w}_k}}{Z^2(\mathbf{x}_i)}\\
        &= \frac{Z(\mathbf{x}_i)}{\e^{\mathbf{x}_i^\top\mathbf{w}_j}}\frac{Z(\mathbf{x}_i)\mathbf{x}_i\e^{\mathbf{x}_i^\top\mathbf{w}_k}\delta_{jk} - \e^{\mathbf{x}_i^\top\mathbf{w}_j}\e^{\mathbf{x}_i^\top\mathbf{w}_k}\mathbf{x}_i}{Z^2(\mathbf{x}_i)}\\
        &= \mathbf{x}_i\delta_{jk} - \mathbf{x}_i\pi_k(\mathbf{x}_i)
    \end{split}
\end{equation*}
zatem
\begin{equation*}
    \pdv{\ell^*}{\mathbf{w}_k} = \gamma\mathbf{w}_k - \sum_{i=1}^m\mathbf{x}_i\sum_{j=1}^s \delta(t_i, \tau_j)\delta_{jk} + \sum_{i=1}^m\mathbf{x}_i\pi_k(\mathbf{x}_i)\sum_{j=1}^s\delta(t_i, \tau_j)\,.
\end{equation*}
Zauważmy jednak, iż
\begin{equation*}
    \sum_{j=1}^s\delta(t_i, \tau_j) = 1\,,\quad \sum_{j=1}^s \delta(t_i, \tau_j)\delta_{jk} = \delta(t_i, \tau_k)\,,
\end{equation*}
zatem ostatecznie
\begin{equation*}
    \pdv{\ell^*}{\mathbf{w}_k} = \gamma\mathbf{w}_k + \sum_{i=1}^m \mathbf{x}_i\left[\pi_k(\mathbf{x}_i) - \delta(t_i, \tau_k)\right]\,.
\end{equation*}
Wprowadzając macierze
\begin{equation*}
    \begin{split}
        &\oper{X} = \mqty[\mathbf{x}_1^\top \\ \vdots \\ \mathbf{x}_m^\top]\,,\quad \oper{W} = \mqty[\mathbf{w}_1^\top \\ \vdots \\ \mathbf{w}_s^\top]\,,\\
        &\oper{S} = \mqty[\pi_1(\mathbf{x}_1) & \cdots & \pi_s(\mathbf{x}_1) \\ \vdots & \ddots & \vdots \\ \pi_1(\mathbf{x}_m) & \cdots & \pi_s(\mathbf{x}_m)]\,,\quad \oper{T} = \mqty[\delta(t_1,\tau_1) & \cdots & \delta(t_1,\tau_s) \\ \vdots & \ddots & \vdots \\ \delta(t_m, \tau_1) & \cdots & \delta(t_m, \tau_s)]
    \end{split}
\end{equation*}
możemy w takim razie zapisać zdefiniowaną wyżej macierz pochodnych wymaganych do algorytmu spadku
wzdłuż gradient w kompaktowej formie jako
\begin{equation*}
    \pdv{\ell^*}{\oper{W}} = (\oper{S} - \oper{T})^\top\oper{X}\,.
\end{equation*}
Zauważmy, iż zregularyzowana funkcja kosztu rośnie wraz ze wzrostem liczby obserwacji \(m\). Wynika
z tego, iż stała ucząca musi być zależna od liczby przykładów. Możemy na przykład stwierdzić, iż
\(\epsilon \leftarrow m^{-1}\epsilon\) i wówczas minimalizujemy tak naprawdę średni koszt \(\ell^* /
m\).

\subsection{Wnioskowanie metodami Monte Carlo}

Całe wnioskowanie Bayesowskie opiera się na wyznaczaniu rozkładów a posteriori, które wyrażają naszą
wiedzę o estymowanym parametrze. Do tej pory rozważaliśmy modele Bayesowskie dla których prior i
wiarygodność były dane przez rozkłady normalne. Dzięki temu mogliśmy wyprowadzić analityczne wzory
na parametry rozkładu a posteriori, który również był rozkładem normalnym. Dla wielu interesujących
modeli nie jesteśmy jednak w stanie tego zrobić (np. w zagadnieniu regresji logistycznej
ograniczyliśmy się jedynie do estymaty punktowej), gdyż obliczenie stałej normalizującej dla
rozkładu \(p(\theta \mid D)\) może wymagać obliczenia całki, której nie jesteśmy w stanie wyrazić w
sposób jawny lub sumy po wykładniczo wielu elementach. Wnioskowanie Bayesowskie można jednak
prowadzić w modelach, w których nie dysponujemy jawnym wzorem na gęstość prawdopodobieństwa rozkładu
a posteriori. Okazuje się, iż do generowania próbek z rozkładu \(p(\theta \mid D)\) wystarcza
znajomość tego rozkładu z dokładnością do stałej normalizującej, a zatem wystarczy znać rozkład
łączny \(p(\theta, D) = p(D\mid\theta)\pi(\theta)\). Generowanie próbek z kolei wystarcza natomiast,
na mocy silnego prawa wielkich liczb, do szacowania wartości średnich dowolnych funkcji estymowanego
parametru \(\theta\). Przypomnijmy, iż na mocy silnego prawa wielkich liczb ciąg średnich
częściowych \((\overline{X}_n)\) ciągu zmiennych losowych \((X_n)\) i.i.d. z rozkładu \(X \sim
\mathcal{D}\) jest zbieżny z prawdopodobieństwem 1 do wartości oczekiwanej \(\mathbb{E}[X]\) tj.
\begin{equation*}
    P\left(\lim_{n \to \infty} \overline{X}_n = \mathbb{E}[X]\right) = 1\,.
\end{equation*}
Wartość oczekiwaną \(\mathbb{E}[X]\) możemy zatem przybliżyć średnią \(\overline{X}_n\) z dużej
ilości próbek.

Wnioskowanie Monte Carlo pozwala nam szacować różne wielkości w tzw. hierarchicznych modelach
Bayesowskich (z ang. \textit{Bayesian hierarchical modeling}). Rozważmy jeszcze raz przykład
regresji liniowej w ujęciu Bayesowskim, ale rozważmy teraz model postaci
\begin{equation*}
    \begin{split}
        \sigma^2 &\sim \mathcal{D}(\lambda)\\
        \mathbf{w} &\sim \mathcal{N}(\boldsymbol{\mu}_0, \oper{\Sigma}_0)\\
        y \mid \mathbf{w}, \sigma^2 &\sim \mathcal{N}(\mathbf{w}^\top\mathbf{x}, \sigma^2)
    \end{split}\quad,
\end{equation*}
gdzie \(\lambda, \boldsymbol{\mu}_0, \oper{\Sigma}_0\) są pewnymi hiper-parametrami. Dla takiego
modelu nie możemy w ogólności znaleźć jawnej postaci rozkładu a posteriori. Jeśli jednak umiemy
generować próbki z rozkładu łącznego
\begin{equation*}
    Z\cdot p(\mathbf{w}, \sigma^2 \mid D) = p(D, \mathbf{w}, \sigma^2) = p(D \mid \mathbf{w},\sigma^2)\pi(\mathbf{w})\pi(\sigma^2)
\end{equation*}
to wszystkie interesujące wielkości możemy oszacować jako odpowiednie średnie. Pozostaje pytanie w
jaki sposób generować próbki ze skomplikowanych rozkładów prawdopodobieństwa, których gęstości znamy
jedynie z dokładnością do stałej normalizującej. Poniżej przedstawimy dwa algorytmy próbkowania:
algorytm IS oraz Metropolisa--Hastingsa będący szczególną realizacją całej rodziny algorytmów
próbkowania zwanych Markov Chain Monte Carlo (MCMC).

\subsubsection{Algorytm Importance Sampling (IS)}

Załóżmy, iż chcemy obliczyć wartość oczekiwaną pewnej funkcji zmiennej losowej \(\mathbf{x}\)
względem skomplikowanego rozkładu prawdopodobieństwa \(p(\mathbf{x})\), który znamy jedynie z
dokładnością do stałej normalizującej
\begin{equation*}
    p(\mathbf{x}) = \frac{1}{Z_p}\tilde{p}(\mathbf{x})
\end{equation*}
tj. szukamy
\begin{equation*}
    \mathbb{E}_p[f(\mathbf{x})] = \int f(\mathbf{x}) p(\mathbf{x})\dd[n]\mathbf{x}\,.
\end{equation*}
Jeśli umiemy generować próbki \(\mathbf{x}\) z innego (prostszego) rozkładu \(q(\mathbf{x})\), który
nazywamy rozkładem proponującym kandydatów (z ang. \textit{proposal distribution}) to możemy zapisać
\begin{equation*}
    \begin{split}
        \mathbb{E}_p[f(\mathbf{x})] &= \int\limits_{\mathbb{R}^n}f(\mathbf{x})p(\mathbf{x})\dd[n]\mathbf{x} = \int\limits_{\mathbb{R}^n}f(\mathbf{x})\frac{p(\mathbf{x})}{q(\mathbf{x})}q(\mathbf{x})\dd[n]\mathbf{x}\\
         &=\mathbb{E}_q\left[f(\mathbf{x}) \frac{p(\mathbf{x})}{q(\mathbf{x})}\right] = \frac{Z_q}{Z_p}\mathbb{E}_q\left[f(\mathbf{x}) \frac{\tilde{p}(\mathbf{x})}{\tilde{q}(\mathbf{x})}\right]\,.
    \end{split}\quad.
\end{equation*}
Zakładamy tutaj, iż nośnik rozkładu \(p\) zawiera się w nośniku \(q\) tj. \(\text{supp}\,p \subseteq
\text{supp}\,q\). Stosunek stałych \(Z_p / Z_q\) również możemy oszacować z próbek z \(q\), gdyż
mamy
\begin{equation*}
    Z_p = \int\limits_{\mathbb{R}^n}\tilde{p}(\mathbf{x})\dd[n]{\mathbf{x}} = Z_q\int\limits_{\mathbb{R}^n}\frac{\tilde{p}(\mathbf{x})}{\tilde{q}(\mathbf{x})} q(\mathbf{x})\dd[n]{\mathbf{x}} = Z_q \mathbb{E}_q\left[\frac{\tilde{p}(\mathbf{x})}{\tilde{q}(\mathbf{x})}\right]\,,
\end{equation*}
skąd ostatecznie
\begin{equation*}
    \mathbb{E}_p[f(\mathbf{x})] = \frac{\mathbb{E}_q\left[f(\mathbf{x}) \frac{\tilde{p}(\mathbf{x})}{\tilde{q}(\mathbf{x})}\right]}{\mathbb{E}_q\left[\frac{\tilde{p}(\mathbf{x})}{\tilde{q}(\mathbf{x})}\right]}\,.
\end{equation*}
Jeśli z rozkładu \(q\) wygenerowaliśmy próbki \(X =
\left\{\mathbf{x}_1,\ldots,\mathbf{x}_m\right\}\) to na mocy silnego prawa wielkich liczb mamy
\begin{equation*}
    \mathbb{E}_p[f(\mathbf{x})] \approx \frac{\sum_{i=1}^m f(\mathbf{x}_i) \frac{\tilde{p}(\mathbf{x}_i)}{\tilde{q}(\mathbf{x}_i)}}{\sum_{i=1}^m \frac{\tilde{p}(\mathbf{x}_i)}{\tilde{q}(\mathbf{x}_i)}} = \sum_{i=1}^m \lambda_i f(\mathbf{x}_i)\,,
\end{equation*}
gdzie
\begin{equation*}
    \lambda_i = \frac{\tilde{p}(\mathbf{x}_i) / \tilde{q}(\mathbf{x}_i)}{\sum_{j=1}^m \tilde{p}(\mathbf{x}_j) / \tilde{q}(\mathbf{x}_j) }\,.
\end{equation*}

Algorytm Importance Sampling jest prostym algorytmem Monte Carlo, który ma jeden zasadniczy problem.
W jaki sposób mamy wybrać rozkład proponujący kandydatów \(q\)? Pewną odpowiedź na to pytanie
sugeruje analiza wariancji statystyki 
\begin{equation*}
    \overline{f}_m(\mathbf{x}_1,\ldots,\mathbf{x}_m) = \frac{1}{m}\sum_{i=1}^m \frac{f(\mathbf{x}_i)p(\mathbf{x}_i)}{q(\mathbf{x}_i)}
\end{equation*}
dla \(\mathbf{x}_i \sim q\) i zakładając dla uproszczenia, iż \(f\) jest funkcją skalarną mamy
\begin{equation*}
    \begin{split}
        \text{Var}[\overline{f}_m] &= \frac{1}{m}\text{Var}_q\left[f(\mathbf{x})\frac{p(\mathbf{x})}{q(\mathbf{x})}\right] = \frac{1}{m}\int\limits_{\mathbb{R}^n}\frac{(f(\mathbf{x})p(\mathbf{x}) - \mu_fq(\mathbf{x}))^2}{q(\mathbf{x})}\dd[n]{\mathbf{x}}\,.
    \end{split}
\end{equation*}
Chcemy oczywiście, aby wariancja była jak najmniejsza, gdyż wówczas mała liczba próbek da dobre
przybliżenie wartości oczekiwanej. Rozkład proponujący kandydatów powinien być zatem proporcjonalny
do \(f(\mathbf{x})p(\mathbf{x})\), co może być trudne do praktycznego zrealizowania.

\subsubsection{Algorytm Metropolisa--Hastingsa}

Cała klasa algorytmów próbkowania MCMC opiera się na idei wyrażenia generowania próbek jako ewolucji
pewnego łańcucha Markowa. Łańcuchem Markowa nazywamy ciąg zmiennych losowych \((X_t)\) o wartościach
w \(\mathbb{R}^n\) taki, że spełnione jest kryterium Markowa
\begin{equation*}
    \forall A \subset \mathbb{R}^n: P(X_t \in A \mid X_{t-1} = \mathbf{x}_{t-1}, \ldots, X_0 = \mathbf{x}_0) = P(X_t \in A \mid X_{t-1} = \mathbf{x}_{t-1})\,.
\end{equation*}
Elementy ciągu nazywamy stanami łańcucha Markowa. Dany łańcuch jest zadany jednoznacznie przez
podanie gęstości prawdopodobieństwa przejścia łańcucha ze stanu \(\mathbf{x} \to \mathbf{y}\), którą
będziemy oznaczać przez \(\pi(\mathbf{y} \mid \mathbf{x})\) (zakładamy, iż prawdopodobieństwo
przejścia jest niezależne od chwili \(t\) -- łańcuch taki nazywamy jednorodnym). Funkcja \(\pi\)
spełnia oczywiście warunek unormowania
\begin{equation*}
    \int\limits_{\mathbb{R}^n} \pi(\mathbf{y} \mid \mathbf{x}) \dd[n]{\mathbf{y}}\,,
\end{equation*} 
istotnie prawdopodobieństwo przejścia gdziekolwiek ze stanu \(\mathbf{x}\) jest równe 1. Będziemy
zakładać dodatkowo, iż \(\forall \mathbf{x},\mathbf{y}\in\mathbb{R}^n: \pi(\mathbf{y} \mid
\mathbf{x}) > 0\). Rozkład \(p(\mathbf{x})\) łańcucha Markowa (tj. rozkład prawdopodobieństwa z
którego losujemy stan łańcucha w danej chwili \(t\)) z daną funkcją przejścia \(\pi\) nazwiemy
rozkładem stacjonarnym tego łańcucha iff
\begin{equation*}
    p(\mathbf{y}) = \int\limits_{\mathbb{R}^n} \pi(\mathbf{y} \mid \mathbf{x})p(\mathbf{x}) \dd[n]{\mathbf{x}}\,.
\end{equation*}
Rozkład stacjonarny danego łańcucha oznaczymy przez \(p^*(\mathbf{x})\). Zauważmy, iż jeśli stan
początkowy łańcucha \(X_0\) pochodzi z rozkładu stacjonarnego \(p^*\) to każdy kolejny stan \(X_t\)
również pochodzi z rozkładu stacjonarnego. Jeśli z kolei stan początkowy pochodzi z jakiegoś innego
rozkładu \(p_0\) to rozkład łańcucha w chwili \(t\) jest dany przez relację rekurencyjną
\begin{equation*}
    p_t(\mathbf{y}) = \int\limits_{\mathbb{R}^n} \pi(\mathbf{y} \mid \mathbf{x})p_{t-1}(\mathbf{x}) \dd[n]{\mathbf{x}}\,,\quad\text{dla \(t > 1\).}
\end{equation*}
Rozkładem granicznym łańcucha Markowa nazwiemy granicę w sensie zbieżności punktowej
\begin{equation*}
    \lim_{t\to\infty} p_t(\mathbf{x})\,.
\end{equation*}
Przy podanych wyżej założeniach istnieje twierdzenie, które mówi iż taki łańcuch Markowa posiada
jednoznaczny rozkład stacjonarny tożsamy z rozkładem granicznym. Ponadto warunkiem wystarczającym,
aby dany rozkład \(p(\mathbf{x})\) był rozkładem stacjonarnym łańcucha Markowa jest
\begin{equation*}
    \forall\mathbf{x}, \mathbf{y} \in \mathbb{R}^n: \pi(\mathbf{y}\mid\mathbf{x}) p(\mathbf{x}) = \pi(\mathbf{x} \mid \mathbf{y}) p(\mathbf{y})\,,
\end{equation*}
co wynika z scałkowania powyższego równania
\begin{equation*}
    \int\limits_{\mathbb{R}^n} \pi(\mathbf{y}\mid\mathbf{x}) p(\mathbf{x}) \dd[n]{\mathbf{x}} = \int\limits_{\mathbb{R}^n} \pi(\mathbf{x} \mid \mathbf{y}) p(\mathbf{y}) \dd[n]{\mathbf{x}} = p(\mathbf{y}) \int\limits_{\mathbb{R}^n} \pi(\mathbf{x} \mid \mathbf{y}) \dd[n]{\mathbf{x}} = p(\mathbf{y})\,.
\end{equation*}
Kryterium to nazywamy kryterium lokalnego balansu (z ang. \textit{detailed balance condition}).

Podstawowa idea wykorzystania łańcuchów Markowa do generowania próbek ze skomplikowanego rozkładu
\(p\) jest więc następująca: tworzymy łańcuch Markowa opisany powyżej, dla którego \(p\) jest
rozkładem stacjonarnym, wówczas rozpoczynając w dowolnym dopuszczalnym stanie początkowym \(X_0\) po
wykonaniu dużej liczby kroków (etap ten nazywamy okresem przejściowym z ang. \textit{burn-in
period}) stan \(X_t\) (dla \(t \gg 1\)) tego łańcucha będzie w przybliżeniu pochodził z rozkładu
granicznego \(p\) (nie jest jednak prosto stwierdzić po jak długim okresie przejściowym przybliżenie
to jest wystarczająco dobre). Aby otrzymać z takiej procedury próbki prawdziwie i.i.d. każda z
próbek musiałaby pochodzić z ponownego uruchomienia takiego łańcucha. Oczywiście jest to
nieefektywne, więc w praktyce generujemy próbki z jednego łańcucha po prostu odrzucając pewne z nich
tak aby uniknąć znaczących korelacji.

Pozostaje pytanie jak skonstruować funkcję przejścia \(\pi(\mathbf{y} \mid \mathbf{x})\) dla danego
rozkładu granicznego \(p(\mathbf{x})\). Podstawową konstrukcję podaje algorytm
Metropolisa--Hastingsa:
\begin{enumerate}
    \item Jako stan początkowy przyjmij dowolną dopuszczalną wartość \(\mathbf{x} \leftarrow
    \mathbf{x}_0\).
    \item Powtarzaj:
    \begin{enumerate}
        \item Będąc w aktualnym stanie \(\mathbf{x}\) z prostego rozkładu proponującego kandydatów
        \(q(\mathbf{y} \mid \mathbf{x})\) wylosuj kandydata \(\mathbf{y}\) na wartość łańcucha w
        kolejnym stanie.
        \item Z prawdopodobieństwem
        \begin{equation*}
            r(\mathbf{y} \mid \mathbf{x}) = \min\left\{1, \frac{p(\mathbf{y})q(\mathbf{x} \mid \mathbf{y})}{p(\mathbf{x})q(\mathbf{y} \mid \mathbf{x})}\right\}
        \end{equation*}
        zaakceptuj kandydata jako nowy stan i przejdź do stanu \(\mathbf{y}\). W przeciwnym razie
        pozostać w stanie \(\mathbf{x}\)
    \end{enumerate}
\end{enumerate}
Funkcja przejścia ma zatem postać
\begin{equation*}
    \pi_\text{MH}(\mathbf{y}\mid\mathbf{x}) = q(\mathbf{y} \mid \mathbf{x}) r(\mathbf{y} \mid \mathbf{x})\,.
\end{equation*}
Pozostaje tylko wykazać, iż spełnione jest kryterium lokalnego balansu. Istotnie mamy
\begin{equation*}
    \begin{split}
        &\pi_\text{MH}(\mathbf{y}\mid\mathbf{x})p(\mathbf{x}) = \min\left\{q(\mathbf{y}\mid\mathbf{x})p(\mathbf{x}), q(\mathbf{x}\mid\mathbf{y})p(\mathbf{y})\right\}\\
        &\pi_\text{MH}(\mathbf{x}\mid\mathbf{y})p(\mathbf{y}) = \min\left\{q(\mathbf{x}\mid\mathbf{y})p(\mathbf{y}), q(\mathbf{y}\mid\mathbf{x})p(\mathbf{x})\right\}
    \end{split}\quad,
\end{equation*}
skąd \(\pi_\text{MH}(\mathbf{y}\mid\mathbf{x})p(\mathbf{x}) =
\pi_\text{MH}(\mathbf{x}\mid\mathbf{y})p(\mathbf{y})\). Zauważmy, iż nie musimy znać
\(p(\mathbf{x})\) z dokładnością do stałej normalizującej, gdyż
\begin{equation*}
    \frac{p(\mathbf{y})}{p(\mathbf{x})} = \frac{\tilde{p}(\mathbf{y})/Z_p}{\tilde{p}(\mathbf{x})/Z_p} =  \frac{\tilde{p}(\mathbf{y})}{\tilde{p}(\mathbf{x})}\,.
\end{equation*}
Poza algorytmem Metropolisa--Hastingsa jest wiele innych algorytmów z rodziny MCMC. Większość z nich
implementuje konkretny sposób generowania (zostawiając resztę struktury) tak, aby zmniejszyć
korelację po okresie przejściowym i przyspieszyć zbieżność. Standardowo wykorzystywanymi algorytmami
z tej klasy są algorytmy HMC (\textit{Hamiltonian Monte Carlo}) oraz NUTS (\textit{No U-Turn
Sampler}). 

\linesep
\newpage

\section{Sieci neuronowe}

\subsection{Tensory}

W przypadku uczenia maszynowego \(k\)-\textit{tensor} będziemy utożsamiali z odwzorowaniem
\begin{equation*}
    X:\mathbb{N}^k\supset I \mapsto \mathbb{K}\,,
\end{equation*}
gdzie \(\mathbb{K}\) jest pewnym ciałem, a \(I\) jest pewnym spójnym zbiorem wielowskaźników tj.
zakładamy, iż 
\begin{equation*}
    \forall \alpha \in
    \{1,\ldots, k\} : i_\alpha > 0 \land (i_1,..,i_\alpha,..,i_k) \in I \implies (i_1,..,i_\alpha -
    1,..,i_k) \in I\,.
\end{equation*}
Jest to zatem po prostu wielowymiarowa tablica, której elementy są skalarami. My będziemy zawsze
przyjmować \(\mathbb{K} = \mathbb{R}\). Miejsce \(\alpha\) wielowskaźnika będziemy nazywali osią.
Wartość elementu tensora \(X\) pod wielowskaźnikiem \((i_1, \ldots, i_k)\) będziemy oznaczać
\(X_{i_1,..,i_k}\). Czasami będziemy utożsamiać zapis \(X_{i_1,..,i_k}\) z odwzorowaniem \(X\), a
nie z jego wartością dla argumentu \((i_1, \ldots, i_k)\), aby podkreślić jakie argumenty przyjmuje.
Jest to tożsame z częstym w matematyce oznaczaniem funkcji jako \(f(x)\) podczas, gdy funkcją jest
\(f\), a \(f(x)\) to jej wartość dla argumentu \(x\).\hfill \break
Dla tensora \(X:\mathbb{N}^k\supset I \mapsto \mathbb{R}\) definiujemy tzw. operator kształtu (z
ang. \textit{shape operator}) \(\sh\)
\begin{equation*}
    \sh(X) := (\sh_1(X), \ldots, \sh_k(X))\,,\quad \sh_\alpha(X) := \text{liczba elementów \(X\) wzdłuż osi \(\alpha\).}
\end{equation*}
Operator kształtu zwraca wektor liczb naturalnych zawierający uogólnienie liczb kolumn/wierszy dla
macierzy. W szczególności dla tablicy jednowymiarowej (tj. 1-tensora) \(T\) zawierającej \(n\)
elementów będziemy stosowali zapis \(\sh(T) = (n,)\).\hfill \break
Zbiór wszystkich tensorów danego kształtu \(\sh=(n_1,\ldots,n_k)\) o elementach rzeczywistych
będziemy oznaczać przez \(\mathbb{T}_{n_1,..,n_k}(\mathbb{R}) = \mathbb{T}_\sh(\mathbb{R})\). Jeśli
będzie to oczywiste z kontekstu (u nas zawsze) to pominiemy człon w nawiasach pisząc po prostu
\(\mathbb{T}_{n_1,..,n_k} = \mathbb{T}_\sh\). Odwzorowanie \(F\) o argumentach i wartościach
tensorowych zdefiniujemy jako \(F: \mathbb{T}_{\sh_1} \mapsto \mathbb{T}_{\sh_2}\). Wówczas wartość
tego odwzorowania dla argumentu \(X\) (\(\sh(X) = \sh_1\)) tj. \(F(X)\) jest tensorem (\(\sh(F(X)) =
\sh_2\)). Ponadto będziemy utożsamiać zapis \((F(X))_{i_1,..,i_k}\) z \(F_{i_1,..,i_k}(X)\).\hfill
\break
W wyrażeniach zawierających iloczyny postaci 
\begin{equation*}
    X_{i_1,..,i_p}Y_{j_1,..,j_q}Z_{k_1,..,k_r}\cdots
\end{equation*}
stosujemy (uproszczoną) konwencję sumacyjną Einsteina tj. dla każdego powtórzonego indeksu w takim
wyrażeniu należy przed wyrażenie postawić znak sumy po tym indeksie (tzw. \textit{indeks
sumacyjny}), przy czym dla zwiększenia przejrzystości zapisu nie piszemy znaków sumy explicite.
Oznacza to w szczególności, iż możemy łatwo definiować nowe tensory jako odpowiednie iloczyny np.
iloczyn macierzy \(A \in \mathbb{T}_{n,m}, B \in \mathbb{T}_{m,l}\) możemy zapisać jako
\begin{equation*}
    C_{ik} = A_{ij}B_{jk}\,.
\end{equation*}
wyrażenie to definiuje nowy tensor \(C \in \mathbb{T}_{n, l}\), którego elementy są dane poprzez
sumę iloczynów odpowiednich elementów \(A, B\). Zauważmy jednakże, iż ponieważ nie wykorzystujemy
wskaźników dolnych i górnych, więc jednakowo poprawnymi wyrażeniami są \(A_{ij}X_i\) oraz
\(A_{ij}X_j\) chociaż w ogólności \(A_{ij}X_i \neq A_{ij}X_j\). Dodatkowo dla funkcji \(f:
\mathbb{R} \mapsto \mathbb{R}\) zapis postaci \(Y_{ijk..} = f(X_{ijk..})\) dla dowolnego tensora
\(X\) definiuje tensor \(Y\), którego wartości są wartościami funkcji \(f\) dla odpowiednich
wartości tensora \(X\). Dodatkowo jeśli \(f: \mathbb{R} \mapsto \mathbb{R}\), a \(F :
\mathbb{T}_{\sh_1} \mapsto \mathbb{T}_{\sh_2}\) to przez zapis \(Y = (f \circ F)(X)\) będziemy
rozumieć 
\begin{equation*}
    Y_{ijk..} = f(F_{ijk..}(X))\,.
\end{equation*}

Ostatnim elementem notacji, który będzie nam potrzebny jest tzw. operator zakresu (z ang.
\textit{range operator}). Przez zapis 
\begin{equation*} 
    X_{i_1:n_1:d_1, i_2:n_2:d_2,..,i_k:n_k:d_k}
\end{equation*} 
będziemy rozumieć \(k\)-tensor będący podtensorem \(X\) powstałym przez wybranie z osi \(\alpha\)
tensora \(X\) \(n_\alpha\) elementów rozpoczynając od \(i_\alpha\) i biorąc co \(d_\alpha\)-ty
element. Wartości tego podtensora będziemy oznaczać jako
\((X_{i_1:n_1:d_1,..,i_k:n_k:d_k})_{j_1,..,j_k}\) Jeśli dla danej osi wybieramy cały zakres to
będziemy pisali sam znak ,,:''. Jeśli z kolei któryś z \(d_\alpha\) będzie wynosił 1 to będziemy
pomijać go w zapisie pisząc tylko \(i:n\).

\subsection{Neuron}

Podstawowym elementem każdej sieci neuronowej jest pojedynczy neuron, który możemy traktować jako
odwzorowanie \(y: \mathbb{T}_n \mapsto \mathbb{R}\) będące złożeniem pewnego odwzorowania
nieliniowego \(\sigma:\mathbb{R} \mapsto \mathbb{R}\) z odwzorowaniem afinicznym
\begin{equation*}
    \begin{split}
        &y: \mathbb{T}_n \mapsto \mathbb{R}\\
        &y = \sigma\left(W_i X_i + b\right)
    \end{split}\quad.
\end{equation*}
W praktyce jako funkcji aktywacji \(\sigma\) (\textit{activation function}) najczęściej używamy
funkcji ReLU, GELU lub funkcji sigmoidalnych:
\begin{equation*}
    \text{ReLU}(x) := \max(0, x)\,,\quad \text{GELU}(x) := x\Phi(x)\,,
\end{equation*}
gdzie \(\Phi(x)\) to dystrybuanta standardowego rozkładu normalnego. Pojedyncze neurony są następnie
łączone w sieci w określony sposób tworząc daną architekturę sieci neuronowej.

\subsection{Sieci MLP}

Opis sieci neuronowych zaczniemy od architektury MLP (z ang. \textit{Multilayer Perceptron}). Sieć
MLP składa się z równoległych warstw neuronów, przy czym połączenia występują tylko między neuronami
w sąsiednich warstwach i nie ma połączeń między neuronami w obrębie jednej warstwy. Pierwszą warstwę
sieci nazywamy warstwą wejściową (z ang. \textit{input layer}), ostatnią -- warstwą wyjściową (z
ang. \textit{output layer}), a pozostałe nazywamy warstwami ukrytymi (z ang. \textit{hidden
layers}).

Zauważmy, iż opisane wcześniej modele regresji liniowej i wieloklasowej regresji logistycznej są
przykładamy najprostszych sieci MLP bez żadnych warstw ukrytych. Ich graficzne reprezentacje jako
sieci MLP zamieszczono na Rysunku \ref{fig:simple-mlp1} i \ref{fig:simple-mlp2}. 

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]

            \foreach \m/\l [count=\y] in {1,2,3,4} \node [every neuron/.try, neuron \m/.try]
                (input-\m) at (0,2.5-\y) {$x_\m$};
            
            \foreach \m [count=\y] in {1} \node [every neuron/.try, neuron \m/.try ] (output-\m) at
                (2,2-\y*2) {$\mu$};
                        
            \foreach \i in {1,...,4} \foreach \j in {1} \draw [->] (input-\i) -- (output-\j);
                
        \end{tikzpicture}
        \caption{Graficzna reprezentacja regresji liniowej jako najprostszej sieci MLP}
        \label{fig:simple-mlp1}
    \end{subfigure}
    \hfil
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]

            \foreach \m/\l [count=\y] in {1,2,3,4} \node [every neuron/.try, neuron \m/.try]
                (input-\m) at (0,2.5-\y) {$x_\m$};
            
            
            \foreach \m [count=\y] in {1,2,3} \node [every neuron/.try, neuron \m/.try ] (output-\m)
                at (2,2-\y) {$\pi_\m$};
            
                        
            \foreach \i in {1,...,4} \foreach \j in {1,...,3} \draw [->] (input-\i) -- (output-\j);
                        
        \end{tikzpicture}
        \caption{Graficzna reprezentacja wieloklasowej regresji logistycznej jako najprostszej sieci MLP}
        \label{fig:simple-mlp2}
    \end{subfigure}
    
\end{figure}

Zauważmy, iż wyjściem sieci są parametry docelowego rozkładu prawdopodobieństwa nad obserwacjami tj.
odpowiednio wartość oczekiwana \(\mu\) w przypadku regresji liniowej i prawdopodobieństwa \(\pi_i\)
każdej z klas rozkładu kategorialnego w przypadku regresji logistycznej. W przypadku regresji
liniowej funkcja aktywacji neuronu w warstwie wyjściowej to po prostu funkcja identycznościowa,
natomiast w przypadku regresji logistycznej jest to funkcja soft-max.

Przejdźmy teraz do matematycznego opisu architektury MLP. Załóżmy, iż sieć składa się z \(H + 2\)
warstw (\(H\) warstw ukrytych) o liczbach neuronów \(N_{in} = N_0\) (warstwa wejściowa), \(N_1,
\ldots, N_H\) (warstwy ukryte), \(N_{out} = N_{H+1}\) (warstwa wyjściowa). Jeśli \(X \in
\mathbb{T}_{N_{h-1}}\) jest wejściem \(h\)-tej warstwy ukrytej to jej wyjście \(Y^h \in
\mathbb{T}_{N_h}\) jest dane przez
\begin{equation*}
    Y_i^h = \sigma\left(W_{ij}^hX_j + B_i^h\right) \Longleftrightarrow Y = \left(\sigma \circ L^h\right)(X)\,,
\end{equation*} 
gdzie \(W_{ij}^h\) jest macierzą wag w \(h\)-tej warstwie ukrytej, a \(B_i^h\) jest wektorem
obciążeń (z ang. \textit{bias}) w tej warstwie (indeks górny nie jest potęgą tylko numeruje
warstwy). Zdefiniowaliśmy tutaj odwzorowanie
\begin{equation*}
    \begin{split}
        &L^h : \mathbb{T}_{N_{h-1}} \mapsto \mathbb{T}_{N_{h}}\\
        &L_i^h(X) = W_{ij}^hX_j + B_i^h
    \end{split}\quad.
\end{equation*}
Bez straty ogólności zakładamy, iż wyjście warstwy wyjściowej jest po prostu obciążoną kombinacją
liniową wejść bez żadnej funkcji nieliniowej. Wyjście z sieci MLP jest zatem dane przez następujące
złożenie
\begin{equation*}
    \begin{split}
        &F: \mathbb{T}_{N_{in}} \mapsto \mathbb{T}_{N_{out}}\\
        &F(X) = \left(L^{H+1} \circ \sigma \circ L^H \circ \cdots \circ \sigma \circ L^2 \circ \sigma \circ L^1\right)(X)
    \end{split}\quad.
\end{equation*}
W takiej postaci sieć MLP przetwarza pojedynczo każdy wektor \(X_j\) należący do zbioru danych
\(\chi = \{(Y_i^k, X_j^k) \mid k \in \{1,\ldots,D\}\}\). Możemy jednak rozszerzyć odwzorowanie \(F\)
w taki sposób, aby sieć przetwarzała od razu tensor \(X_{jk}\) zawierający wszystkie przykłady ze
zbioru uczącego
\begin{equation*}
    \begin{split}
        &F: \mathbb{T}_{N_{in}, D} \mapsto \mathbb{T}_{N_{out}, D}\\
        &F(X) = \left(L^{H+1} \circ \sigma \circ L^H \circ \cdots \circ \sigma \circ L^2 \circ \sigma \circ L^1\right)(X)\\
        &\\
        &L^h: \mathbb{T}_{N_{h-1}, D} \mapsto \mathbb{T}_{N_{h}, D}\\
        &L_{ik}^h(X) = W_{ij}^h X_{jk} + B_i^h I_k
    \end{split}\quad.
\end{equation*}
gdzie \(\text{\reflectbox{$\mathbb{D}$}}_I = \{1\}\).

Wyjście z sieci neuronowej będziemy wykorzystywać przy tworzeniu modeli statystycznych w taki sam
sposób jak w modelach płytkich (regresja liniowa, regresja softmax) używaliśmy kombinacji liniowej
wektora obserwacji z wektorem wag \(\mathbf{w}^\top\mathbf{x}\). Uczenie parametrów \(W_{ij}^h,
B_i^h\) będzie zatem polegało na znalezieniu ich estymaty punktowej MLE dla danego modelu
statystycznego. Będziemy zatem minimalizować pewien funkcjonał \(J\) (funkcję kosztu) o postaci
danej przez zanegowaną logarytmiczną funkcję wiarygodności. Przykładowo dla regresji użyjemy
funkcjonału kwadratowego
\begin{equation*}
    J[F] = \frac{1}{2}\left(F_{ij}(X) - Y_{ij}\right)^2\,,
\end{equation*}
gdzie \((Y_{ik}, X_{jk})\) jest zbiorem danych treningowych. Natomiast w przypadku wieloklasowej
regresji logistycznej jako funkcjonał przyjmiemy entropię krzyżową
\begin{equation*}
    J[F] = -T_{ik}\log \Pi_{ik}\,,\quad \Pi_{ik} = \frac{\exp\left\{F_{ik}(X)\right\}}{I_j\exp\left\{F_{jk}(X)\right\}}\,,
\end{equation*}
gdzie \(T_{:,k}\) jest wektorem w postaci \textit{one-hot encoding} prawidłowej klasy dla wektora
zmiennych objaśniających \(X_{:,k}\) ze zbioru danych treningowych.

\subsection{Wsteczna propagacja błędu}

W przypadku algorytmów minimalizacji takich jak spadek wzdłuż gradientu musimy znać wartości
pochodnych funkcji kosztu po parametrach modelu. Wyznaczymy teraz te pochodne dla wag w sieci MLP.
Niech \(Y_{ab}^h = \sigma(L_{ab}^h)\), wówczas \(L_{ab}^h = W_{ac}^hY_{cb}^{h-1} + B_a^hI_b\), przy
czym zakładamy \(Y^0 = X\). Mamy odpowiednio
\begin{equation*}
    \begin{split}
        &\pdv{L_{ab}^h}{W_{ef}^h} = Y_{cb}^{h-1}\delta_{ae}\delta_{cf} = Y_{fb}^{h-1}\delta_{ae}\\
        &\pdv{L_{ab}^h}{Y_{ef}^{h-1}} = W_{ac}^h\delta_{ce}\delta_{fb} = W_{ae}^h\delta_{fb}\\
        &\pdv{Y_{ab}^h}{L_{cd}^{h}} = \sigma'(L_{ab}^h)\delta_{ac}\delta_{bd}
    \end{split}\quad.
\end{equation*}
Jednocześnie zauważmy, że
\begin{equation*}
    \pdv{J}{W_{ab}^h} = \left(\pdv{J}{F_{cd}}\pdv{F_{cd}}{L_{ef}^{H+1}}\right)\left(\pdv{L_{ef}^{H+1}}{Y_{ij}^H}\pdv{Y_{ij}^H}{L_{kl}^H}\right)\left(\pdv{L_{kl}^H}{Y_{mn}^{H-1}}\pdv{Y_{mn}^{H-1}}{L_{pq}^{H-1}}\right)\bigg(\cdots\bigg)\pdv{L_{xy}^h}{W_{ab}^h}\,.
\end{equation*}
Pochodne możemy obliczać zatem iteracyjnie jako
\begin{equation*}
    \pdv{J}{W_{ab}^h} = \Delta^h_{ij}\pdv{L_{ij}^h}{W_{ab}^h} = \Delta^h_{ij}Y_{bj}^{h-1}\delta_{ai} = \boxed{\Delta_{aj}^hY_{bj}^{h-1}}\,,
\end{equation*}
gdzie \(\Delta_{ij}^h\) wyznaczamy iteracyjnie jako
\begin{equation*}
    \begin{split}
        &\Delta^{H+1}_{ij} = \pdv{J}{F_{kl}}\pdv{F_{kl}}{L_{ij}^{H+1}} = \pdv{J}{F_{kl}}\delta_{ik}\delta_{jl} = \boxed{\pdv{J}{F_{ij}}}\\
        &\Delta^{h}_{ij} = \Delta_{kl}^{h+1}\pdv{L^{h+1}_{kl}}{Y_{mn}^h}\pdv{Y_{mn}^h}{L_{ij}^h} = \Delta_{kl}^{h+1} W^{h+1}_{km}\delta_{ln}\sigma'(L_{mn}^h)\delta_{mi}\delta_{nj} = \boxed{\Delta_{kj}^{h+1}W_{ki}^{h+1}\sigma'(L_{ij}^h)}
    \end{split}
\end{equation*}
Zauważmy również, iż wzór na pochodną funkcji kosztu \(J\) po parametrach \(B^h_a\) możemy od razu
zapisać jako
\begin{equation*}
    \pdv{J}{B_{a}^h} = \Delta_{aj}^hI_j\,.
\end{equation*}

Możemy zatem zapisać algorytm obliczania pochodnych funkcji kosztu po parametrach sieci neuronowej
zwany algorytmem wstecznej propagacji błędu (z ang. \textit{error backpropagation})

\begin{tcolorbox}[title=Algorytm wstecznej propagacji błędu]
    \begin{enumerate}
        \item Dla zbioru danych \((Y_{pq}, X_{rq})\) dokonaj propagacji naprzód (z ang
        \textit{forward propagation}) sieci MLP i zapamiętaj wartości \(Y_{bj}^{h-1}\) oraz
        \(L_{ij}^h\).

        \item Wyznacz rekurencyjnie i zapamiętaj wartości \(\Delta_{ij}^h\) korzystając ze wzorów
        (etap \textit{backpropagation})
        \begin{equation*}
            \boxed{\Delta_{ij}^{H+1} = \pdv{J}{F_{ij}}\,,\quad \Delta_{ij}^h = \Delta_{kj}^{h+1}W_{ki}^{h+1}\sigma'(L_{ij}^h)}\,.
        \end{equation*}

        \item Wyznacz wartości pochodnych funkcji kosztu po parametrach sieci korzystając z
        \begin{equation*}
            \boxed{\pdv{J}{W_{ab}^h} = \Delta_{aj}^h Y_{bj}^{h-1}\,,\quad \pdv{J}{B_a^h} = \Delta_{aj}^hI_j}\,,
        \end{equation*}
        pamiętając, iż \(Y^0 = X\).
    \end{enumerate}
\end{tcolorbox}

Obliczmy jeszcze pochodne \(\partial J/\partial F_{ab}\) dla problemów regresji i klasyfikacji. W
przypadku regresji sprawa jest prosta
\begin{equation*}
    \begin{split}
        &J = \frac{1}{2}\left(F_{ij} - Y_{ij}\right)^2\\
        &\pdv{J}{F_{ab}} = \left(F_{ij} - Y_{ij}\right)\delta_{ai}\delta_{bj} = F_{ab} - Y_{ab}\,.
    \end{split}
\end{equation*}
W przypadku entropii krzyżowej rachunki są trochę bardziej zawiłe:
\begin{equation*}
    \begin{split}
        &J = -T_{ik}\log\Pi_{ik}\,,\quad \Pi_{ik} = \frac{\exp F_{ik}}{I_j \exp F_{jk}}\,,\\
        &\pdv{J}{F_{ab}} = \pdv{J}{\Pi_{cd}}\pdv{\Pi_{cd}}{F_{ab}}\,,
    \end{split}
\end{equation*}
dalej
\begin{equation*}
    \pdv{J}{\Pi_{cd}} = -\frac{T_{ik}}{\Pi_{ik}}\delta_{ic}\delta_{kd} = - \frac{T_{cd}}{\Pi_{cd}}
\end{equation*}
oraz
\begin{equation*}
    \begin{split}
        \pdv{\Pi_{cd}}{F_{ab}} &= \frac{\exp(F_{cd})\delta_{ac}\delta_{bd}I_j\exp(F_{jd}) - \exp(F_{cd})I_j\exp(F_{jd})\delta_{aj}\delta_{bd}}{[I_j \exp(F_{jd})]^2}\\
        &= \frac{\exp(F_{cd})\delta_{ac}\delta_{bd}}{I_j\exp(F_{jd})} - \frac{\exp(F_{cd})I_a\exp(F_{ad})\delta_{bd}}{[I_j \exp(F_{jd})]^2}\,,
    \end{split}
\end{equation*}
skąd
\begin{equation*}
    \begin{split}
        \pdv{J}{F_{ab}} &= - \frac{T_{cd}}{\Pi_{cd}}\frac{\exp(F_{cd})\delta_{ac}\delta_{bd}}{I_j\exp(F_{jd})} + \frac{T_{cd}}{\Pi_{cd}}\frac{\exp(F_{cd})I_a\exp(F_{ad})\delta_{bd}}{[I_j \exp(F_{jd})]^2} \\
        &= -\frac{T_{ab}}{\Pi_{ab}}\frac{\exp F_{ab}}{I_j \exp F_{jb}} + \frac{T_{cb}}{\Pi_{cb}}\frac{I_a \exp(F_{cb})\exp(F_{ab})}{[I_j \exp(F_{jb})]^2}\\
        &= -\frac{T_{ab}}{\Pi_{ab}}\Pi_{ab} + \frac{T_{cb}}{\Pi_{cb}}I_a \Pi_{cb}\Pi_{ab}\\
        &= - T_{ab}I_{ab} + T_{cb}I_{cb}I_a\Pi_{ab}
    \end{split}
\end{equation*}
ale z definicji \(T\) mamy, iż \(\sum_{c}T_{cb}I_{cb} = I_b\), gdyż dla danego przykładu \(b\)
dokładne jedna klasa \(c\) jest poprawna
\begin{equation*}
    \pdv{J}{F_{ab}} = - T_{ab}I_{ab} + I_aI_b\Pi_{ab} = \Pi_{ab} - T_{ab}\,.
\end{equation*}

\subsection{Automatyczne różniczkowanie}

\subsection{Metody optymalizacji numerycznej}

Mając algorytm efektywnego obliczania pochodnych funkcji kosztu, funkcję minimalizujemy korzystając
z jednego z algorytmów optymalizacji numerycznej pierwszego rzędu (tj. wykorzystującego jedynie
pochodne 1-go rzędu). W przypadku sieci neuronowych funkcja kosztu nie jest funkcją ściśle wypukłą,
więc algorytmy te nie znajdą w ogólności globalnego minimum; możemy liczyć jedynie na znalezienie
minimum lokalnego.

\begin{tcolorbox}[title=Algorytm MBGD]
\begin{enumerate}
    \item Zainicjuj parametry \(W^h_{ab}, B_a^h\) niewielkimi wartościami losowymi.

    \item Podziel dane treningowe na \(K\) rozłącznych mini-batchy \(D_1, \ldots, D_K\) jednakowej
    wielkości.
    
    \item Powtarzaj przez \(N\) epok: \\
    Dla każdego mini-batcha \(D\):
    \begin{enumerate}
        \item Oblicz pochodne funkcji kosztu \(J\) po parametrach sieci korzystając z algorytmu
        wstecznej propagacji błędu.

        \item Zaktualizuj wartości parametrów zgodnie z:
        \begin{equation*}
            \boxed{
            \begin{split}
                &W^{h}_{ab} \leftarrow W^{h}_{ab} - \frac{\epsilon}{|D|}\pdv{J}{W_{ab}^h}\\
                &B_a^h \leftarrow B_a^h - \frac{\epsilon}{|D|}\pdv{J}{B_a^h}
            \end{split}}\quad,
        \end{equation*}
        gdzie \(\epsilon\) jest hiperparametrem zwanym stałą uczącą (z ang. \textit{learning rate}).
    \end{enumerate}
\end{enumerate}
\end{tcolorbox}

\subsection{Regularyzacja}

Sieci neuronowe są bardzo \textit{pojemnymi} modelami, przez co są bardzo podatne na overfitting i
nie generalizują się dobrze poza zbiór treningowy. Bardzo ważnym elementem budowy sieci neuronowych
są więc metody regularyzacji. 

\begin{enumerate}
    \item \textbf{Czynniki regularyzujące}\\
    Analogicznie jak w przypadku prostych modeli liniowych jedną z możliwości regularyzacji jest
    dodanie do funkcji kosztu \(J\) czynnika regularyzującego zawierającego odpowiednią sumę norm
    poszczególnych parametrów sieci
    \begin{equation*}
        \begin{split}
            &J^* = J + \frac{\lambda}{2}\sum_{h=1}^{H+1}\sum_{i,j} \left(W_{ij}^h\right)^2\,,\quad\text{regularyzacja L2}\\
            &J^* = J + \frac{\lambda}{2}\sum_{h=1}^{H+1}\sum_{i,j} \left|W_{ij}^h\right|\,,\quad\text{regularyzacja L1}
        \end{split}
    \end{equation*}
    Różnica między regularyzacją L2 i L1 jest taka, że norma L2 ,,zachęca” sieć do wybierania małych
    wag, ale nie dokładnie będących 0, natomiast L1 zmniejsza wagi, ale w szczególności prowadzi do
    wag wynoszących dokładnie 0. W związku z tym waga L1 przeprowadza selekcję cech - “wyłączy” te,
    które są mało ważne.

    \item \textbf{Early stopping}\\
    Innym prostym, lecz efektywnym podejściem do regularyzacji sieci neuronowych jest procedura
    \textit{early stopping}. Polega ona na zatrzymaniu procesu uczenia (optymalizacji funkcji
    kosztu) w momencie, w którym wartość funkcji kosztu na wydzielonym ze zbioru treningowego
    zbiorze walidacyjnym zaczyna rosnąć (lub nie spada przez odpowiednio długi czas). Unikamy w ten
    sposób przeuczenia modelu. Takie podejście wymaga jednak dość dużego zbioru danych, z których
    musimy wydzielić trzy zbiory: zbiór treningowy (na którym uczymy model), zbiór walidacyjny (na
    którym ewaluujemy model przy procedurze early stoppingu) oraz zbiór testowy (na którym
    ewaluujemy model po treningu). Okres, przez który czekamy na uzyskanie lepszego wyniku, to
    cierpliwość (z ang. \textit{patience}). Im mniejsze, tym mocniejszy jest ten rodzaj
    regularyzacji, ale trzeba z tym uważać, bo łatwo jest przesadzić i zbyt szybko przerywać
    trening. Niektóre implementacje uwzględniają tzw. \textit{grace period}, czyli gwarantowaną
    minimalną liczbę epok, przez którą będziemy trenować sieć, niezależnie od wybranej cierpliwości.

    \item \textbf{Dropout}\\
    Bardzo popularną i szeroko stosowaną metodą regularyzacji sieci neuronowych jest dropout
    polegający na tym, iż w procesie treningu ze z góry zadanym prawdopodobieństwem ,,wyłączamy''
    wyjście neuronu (tj. dla każdej aktualizacji wartości parametrów sieci, w algorytmie wstecznej
    propagacji błędu mnożymy wyjście \(Y_{ab}^h\) przez tensor \(P_{ab}^h\), którego każdy element
    może być równy \(0\) z p-stwem \(p\) lub 1 z p-stwem \(1-p\)). Mechanizmu dropout używamy tylko
    w procesie treningu. W przypadku inferencji wyjście neuronu jest z kolei mnożone przez
    następujący stosunek
    \begin{equation*}
        1 - \frac{\# \text{wyłączeń neuronu}}{\# \text{liczba aktualizacji parametrów sieci}}\,.
    \end{equation*}
    Typowo usuwa się dość sporo neuronów w warstwach ukrytych, np. 50\%. Dzięki dropoutowi można
    używać z nim dość dużych stałych uczących oraz współczynników pędu, a także trenować znacznie
    większe i głębsze sieci. Okazuje się, iż w praktyce takie duże, mocno trenowane sieci z
    regularyzacją uczą się lepiej niż mniejsze, które by tej regularyzacji nie potrzebowały.
    
    \item \textbf{Połączenia rezydualne}\\
    Sporym problemem w procesie uczenia sieci neuronowych jest fakt, iż funkcja kosztu \(J\) nie
    jest wypukła, a dodatkowo często nie jest dość regularna tj. pojawiają się tzw. ,,poszarpania''
    (z ang. \textit{shattered}), które powodują problemy z obliczaniem gradientu. Metodą, która
    pozawala do pewnego stopnia wyeliminować ten problem jest wprowadzenie w sieci połączeń
    rezydualnych tj. warstw, których wyjście jest dane przez
    \begin{equation*}
        \begin{split}
            &Y^h: \mathbb{T}_{N, D} \mapsto \mathbb{T}_{N,D}\\
            &Y^h_{ik}(X) = \sigma\left(W_{ij}^h X_{jk} + B_{i}^h I_k\right) + X_{ik}
        \end{split}\quad.
    \end{equation*}
    Oczywiście wprowadzenie takiej warstwy oznacza, iż liczba neuronów w następnie warstwie musi być
    taka sama.

    \item \textbf{Data augmentation}\\
    Metoda data augmentation polega na dodaniu do zbioru treningowego replik przykładów z tego
    zbioru poddanych pewnym transformacjom względem których oczekujemy, iż odpowiedzi sieci będą
    niezmienione.

\end{enumerate}

\subsection{Sieci CNN}

Sieci konwolucyjne (CNN) to architektura sieci neuronowych przystosowana do przetwarzania danych
posiadających pewne zależności "przestrzenne", przez co rozumiemy, iż jeśli \(X_{ijk}\) jest
tensorem wejściowym to zależności występują między wartościami sąsiednimi \(X_{i,j,k},
X_{i+a,j+b,k+c}\) dla niewielkich \(a,b,c\). Przykładem takich danych są oczywiście obrazy.
Przetwarzanie takich danych przez sieć MLP byłoby niezwykle nieefektywne, gdyż połączenia każdy z
każdym oznaczałyby iż przykładowo dla obrazu w skali szarości o rozdzielczości \(512 \times 512\)
mielibyśmy rzędu \(10^9\) parametrów dla jednej warstwy ukrytej. Dodatkowo sieć MLP szukałaby
powiązań między odległymi pikselami, pomiędzy którymi takich zależności nie ma. Sieć CNN robi to
znacznie efektywniej wykorzystując warstwy konwolucyjne. Jeśli \(X_{ijk}\) jest tensorem wejściowym
do warstwy konwolucyjnej (np. pojedynczym obrazem reprezentowanym przez 3-tensor o kształcie \((512,
512, 3)\)) to wyjście warstwy \(C_{abc}\) jest opisane przez funkcję (\textbf{uwaga} zakładamy, iż
indeksy są liczbami naturalnymi \textbf{bez} 0)
\begin{equation*}
    \begin{split}
        &C: \mathbb{T}_{h_{in},w_{in},c_{in}} \mapsto \mathbb{T}_{h_{out},w_{out},c_{out}}\\
        &C_{abc}(X) = (X_{s(a-1)+1:h:d,s(b-1)+1:w:d,:})_{ijk}W_{ijkc} + B_c I_a I_b\,,    
    \end{split}
\end{equation*}
gdzie \(\sh(W) = (h, w, c_\text{in}, c_\text{out})\) oraz \(c_\text{in} = \sh_3(X)\). Tensor
\(W_{ijkc}\) dla ustalonego \(c\) nazywany \textit{filtrem konwolucyjnym}, natomiast cały tensor
\(W\) -- \textit{mapą konwolucyjną} (formalnie nie jest to operacja konwolucji tylko korelacja
skrośna). Warstwę konwolucyjną charakteryzuje szereg parametrów:
\begin{itemize}
    \item liczba kanałów wejściowych \(c_\text{in} = \sh_3(X)\) (oś 3. nazywamy również wymiarem
    głębi)
    \item liczba kanałów wyjściowych równa liczbie filtrów \(c_\text{out}\)
    \item wysokość i szerokość tzw. \textit{pola odbiorczego} \(h \times w\)
    \item \textit{stride} \(s\), zwykle jednakowy dla wymiaru góra-dół i lewo-prawo chociaż w
    ogólności możemy przyjąć różne wartości \(s_h, s_w\). Parametr ten określa o ile przesuwamy pole
    odbiorcze w każdym kroku.
    \item \textit{dilation} \(d\), ponownie zwykle jednakowy dla obu wymiarów chociaż w ogólności
    możemy przyjąć różne wartości \(d_h, d_w\)
\end{itemize}
Zależność między kształtami wejściowym \((h_\text{in},w_\text{in},c_\text{in})\) i wyjściowym
\((h_\text{out},w_\text{out},c_\text{out})\) dla dowolnych parametrów \(h,w,s,d,c_\text{out}\) można
zapisać jako 
\begin{equation*}
    h_\text{out} = \left\lfloor \frac{h_\text{in} - d_h(h-1) - 1}{s_h} + 1\right\rfloor\,,\quad w_\text{out} = \left\lfloor \frac{w_\text{in} - d_w(w-1) - 1}{s_w} + 1\right\rfloor\,.
\end{equation*}
Do algorytmu wstecznej propagacji błędu potrzebujemy znać pochodne wyjścia warstwy konwolucyjnej po
parametrach sieci. Oczywiście pochodna po parametrach tej warstwy jest oczywista do obliczenia,
podamy więc jedynie pochodną elementu podtensora \(X\) po pewnym elemencie tego tensora
\begin{equation*}
    \pdv{(X_{a:n:p,b:m:q,c:l:r})_{ijk}}{X_{xyz}} = \delta_{a+(i-1)p,x}\delta_{a+(j-1)q,y}\delta_{c+(k-1)r,z}\,.
\end{equation*}
Analogicznie jak w przypadku sieci MLP możemy rozszerzyć definicję funkcji \(C\) definiującej
warstwę konwolucyjną w taki sposób, aby przetwarzała od razu cały 4-tensor zawierający pojedyncze
przykłady będące 3-tensorami (np. kolorowe obrazy)
\begin{equation*}
    \begin{split}
        &C: \mathbb{T}_{h_{in}, w_{in}, c_{in}, D} \mapsto \mathbb{T}_{h_{out}, w_{out}, c_{out}, D}\\
        &C_{abce}(X) = (X_{s(a-1)+1:h:d,s(b-1)+1:w:d,:,:})_{ijke}W_{ijkc} + B_c I_a I_b I_e
    \end{split}
\end{equation*}
W powyższej definicji stosujemy tzw. zapis \textit{batch-last}, tzn. oś (wymiar) związany z różnymi
przykładami w zbiorze uczącym (\textit{batch dimension}) jest ostatnim wymiarem tensora.

Zauważmy, że dany filtr konwolucyjny (ustalone \(c\)) ma te same wartości (po treningu) niezależnie
od położenia pola odbiorczego w wolumenie wejściowym. Filtr taki w takim razie wykrywa konkretną
(ale nie zakodowaną ręcznie tylko wytrenowaną w procesie uczenia sieci) cechę.

Drugim rodzajem warstw typowo wykorzystywanym w sieciach CNN są tzw. \textit{warstwy poolingowe}
(zbierające). Są to warstwy bezparametrowe, które z pola odbiorczego wyciągają pojedynczą liczbę.
Najczęściej stosowane są warstwy typu max pooling wyciągające wartość największą lub average
pooling, które biorą średnią z wartości w polu odbiorczym. Zwykle parametry \(d, s\) dobiera się w
taki sposób, aby kolejne pola odbiorcze nie zachodziły na siebie, choć nie jest to żadna reguła.
Warstwy poolingowe nie zmieniają wymiaru głębi, ale pozwalają zredukować wymiar przestrzenny.

\subsection{Mechanizmy atencji}

\subsection{Sieci Transformer}

\subsection{Generatywne modele dyfuzyjne}

% ---

\section{Reinforcement learning}

\end{document}
